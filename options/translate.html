

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="EN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="EN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Translate &mdash; OpenNMT-py  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
        <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Server" href="server.html" />
    <link rel="prev" title="Train" href="train.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> OpenNMT-py
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changes.html">Versions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CONTRIBUTING.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ref.html">References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Frequently Asked Questions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html">How do I use my v2 models in v3 ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-do-i-train-the-transformer-model">How do I train the Transformer model?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#performance-tips">Performance tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#position-encoding-absolute-vs-relative-vs-rotary-embeddings-vs-alibi">Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#do-you-support-multi-gpu">Do you support multi-gpu?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-do-i-use-pretrained-embeddings-e-g-glove">How do I use Pretrained embeddings (e.g. GloVe)?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-ensemble-models-at-inference">How can I ensemble Models at inference?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-weight-different-corpora-at-training">How can I weight different corpora at training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#what-special-tokens-does-opennmt-py-use">What special tokens does OpenNMT-py use?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-apply-on-the-fly-tokenization-and-subword-regularization-when-training">How can I apply on-the-fly tokenization and subword regularization when training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms">What are the readily available on-the-fly data transforms?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-create-custom-on-the-fly-data-transforms">How can I create custom on-the-fly data transforms?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-to-use-lora-and-8bit-loading-to-finetune-a-big-model">How to use LoRa and 8bit loading to finetune a big model ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-to-use-gradient-checkpointing-when-dealing-with-a-big-model">How to use gradient checkpointing when dealing with a big model ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#can-i-get-word-alignments-while-translating">Can I get word alignments while translating?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-update-a-checkpoint-s-vocabulary">How can I update a checkpoint’s vocabulary?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-use-source-word-features">How can I use source word features?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-set-up-a-translation-server">How can I set up a translation server ?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../examples/wmt17/Translation.html">Translation WMT17 en-de</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/wiki_103/LanguageModelGeneration.html">Language Model Wiki-103</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/summary/Summarization.html">Summarization CNN/DM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/ggnn/GGNN.html">Gated Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/replicate_vicuna/ReplicateVicuna.html">Supervised Finetuning of llama 7B to replicate Vicuna</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Scripts</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="build_vocab.html">Build Vocab</a></li>
<li class="toctree-l1"><a class="reference internal" href="train.html">Train</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Translate</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#Configuration">Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Model">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Data">Data</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Features">Features</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Beam Search">Beam Search</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Random Sampling">Random Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Reproducibility">Reproducibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Penalties">Penalties</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Decoding tricks">Decoding tricks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Logging">Logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Distributed">Distributed</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Efficiency">Efficiency</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/InsertMaskBeforePlaceholdersTransform">Transform/InsertMaskBeforePlaceholdersTransform</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Uppercase">Transform/Uppercase</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/InlineTags">Transform/InlineTags</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/BART">Transform/BART</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Terminology">Transform/Terminology</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Docify">Transform/Docify</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/InferFeats">Transform/InferFeats</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Filter">Transform/Filter</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Prefix">Transform/Prefix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Suffix">Transform/Suffix</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/FuzzyMatching">Transform/FuzzyMatching</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Clean">Transform/Clean</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/SwitchOut">Transform/SwitchOut</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Token_Drop">Transform/Token_Drop</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Token_Mask">Transform/Token_Mask</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Subword/Common">Transform/Subword/Common</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Subword/ONMTTOK">Transform/Subword/ONMTTOK</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Transform/Normalize">Transform/Normalize</a></li>
<li class="toctree-l2"><a class="reference internal" href="#Quant options">Quant options</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="server.html">Server</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../onmt.html">Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.translation.html">Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.translate.translation_server.html">Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.inputters.html">Data Loaders</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Legacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../legacy/FAQ.html">FAQ (Legacy version)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legacy/im2text.html">Image to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legacy/speech2text.html">Speech to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legacy/vid2text.html">Video to Text</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OpenNMT-py</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Translate</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/options/translate.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="translate">
<h1>Translate<a class="headerlink" href="#translate" title="Permalink to this heading">¶</a></h1>
<p><p>translate.py</p>
</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">usage</span><span class="p">:</span> <span class="n">translate</span><span class="o">.</span><span class="n">py</span> <span class="p">[</span><span class="o">-</span><span class="n">h</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">config</span> <span class="n">CONFIG</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">save_config</span> <span class="n">SAVE_CONFIG</span><span class="p">]</span> <span class="o">--</span><span class="n">model</span>
                    <span class="n">MODEL</span> <span class="p">[</span><span class="n">MODEL</span> <span class="o">...</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">precision</span> <span class="p">{,</span><span class="n">fp32</span><span class="p">,</span><span class="n">fp16</span><span class="p">,</span><span class="n">int8</span><span class="p">}]</span> <span class="p">[</span><span class="o">--</span><span class="n">fp32</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">int8</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">avg_raw_probs</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">self_attn_type</span> <span class="n">SELF_ATTN_TYPE</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">data_type</span> <span class="n">DATA_TYPE</span><span class="p">]</span>
                    <span class="o">--</span><span class="n">src</span> <span class="n">SRC</span> <span class="p">[</span><span class="o">--</span><span class="n">tgt</span> <span class="n">TGT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">tgt_file_prefix</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">output</span> <span class="n">OUTPUT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">report_align</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">gold_align</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">report_time</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">profile</span><span class="p">]</span> <span class="p">[</span><span class="o">-</span><span class="n">n_src_feats</span> <span class="n">N_SRC_FEATS</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">src_feats_defaults</span> <span class="n">SRC_FEATS_DEFAULTS</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">beam_size</span> <span class="n">BEAM_SIZE</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">ratio</span> <span class="n">RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">random_sampling_topk</span> <span class="n">RANDOM_SAMPLING_TOPK</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">random_sampling_topp</span> <span class="n">RANDOM_SAMPLING_TOPP</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">random_sampling_temp</span> <span class="n">RANDOM_SAMPLING_TEMP</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">seed</span> <span class="n">SEED</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">length_penalty</span> <span class="p">{</span><span class="n">none</span><span class="p">,</span><span class="n">wu</span><span class="p">,</span><span class="n">avg</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">alpha</span> <span class="n">ALPHA</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">coverage_penalty</span> <span class="p">{</span><span class="n">none</span><span class="p">,</span><span class="n">wu</span><span class="p">,</span><span class="n">summary</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">beta</span> <span class="n">BETA</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">stepwise_penalty</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">min_length</span> <span class="n">MIN_LENGTH</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">max_length</span> <span class="n">MAX_LENGTH</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">max_length_ratio</span> <span class="n">MAX_LENGTH_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">block_ngram_repeat</span> <span class="n">BLOCK_NGRAM_REPEAT</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">ignore_when_blocking</span> <span class="n">IGNORE_WHEN_BLOCKING</span> <span class="p">[</span><span class="n">IGNORE_WHEN_BLOCKING</span> <span class="o">...</span><span class="p">]]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">replace_unk</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">ban_unk_token</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">phrase_table</span> <span class="n">PHRASE_TABLE</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">log_file</span> <span class="n">LOG_FILE</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">log_file_level</span> <span class="p">{</span><span class="n">CRITICAL</span><span class="p">,</span><span class="n">ERROR</span><span class="p">,</span><span class="n">WARNING</span><span class="p">,</span><span class="n">INFO</span><span class="p">,</span><span class="n">DEBUG</span><span class="p">,</span><span class="n">NOTSET</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="mi">30</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">0</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">verbose</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">attn_debug</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">align_debug</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">dump_beam</span> <span class="n">DUMP_BEAM</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">n_best</span> <span class="n">N_BEST</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">with_score</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">gpu_ranks</span> <span class="p">[</span><span class="n">GPU_RANKS</span> <span class="o">...</span><span class="p">]]</span> <span class="p">[</span><span class="o">--</span><span class="n">world_size</span> <span class="n">WORLD_SIZE</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">parallel_mode</span> <span class="p">{</span><span class="n">tensor_parallel</span><span class="p">,</span><span class="n">data_parallel</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">gpu_backend</span> <span class="n">GPU_BACKEND</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">gpu_verbose_level</span> <span class="n">GPU_VERBOSE_LEVEL</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">master_ip</span> <span class="n">MASTER_IP</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">master_port</span> <span class="n">MASTER_PORT</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">timeout</span> <span class="n">TIMEOUT</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">batch_size</span> <span class="n">BATCH_SIZE</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">batch_type</span> <span class="p">{</span><span class="n">sents</span><span class="p">,</span><span class="n">tokens</span><span class="p">}]</span> <span class="p">[</span><span class="o">--</span><span class="n">gpu</span> <span class="n">GPU</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">transforms</span> <span class="p">{</span><span class="n">insert_mask_before_placeholder</span><span class="p">,</span><span class="n">uppercase</span><span class="p">,</span><span class="n">inlinetags</span><span class="p">,</span><span class="n">bart</span><span class="p">,</span><span class="n">terminology</span><span class="p">,</span><span class="n">docify</span><span class="p">,</span><span class="n">inferfeats</span><span class="p">,</span><span class="n">filtertoolong</span><span class="p">,</span><span class="n">prefix</span><span class="p">,</span><span class="n">suffix</span><span class="p">,</span><span class="n">fuzzymatch</span><span class="p">,</span><span class="n">clean</span><span class="p">,</span><span class="n">switchout</span><span class="p">,</span><span class="n">tokendrop</span><span class="p">,</span><span class="n">tokenmask</span><span class="p">,</span><span class="n">sentencepiece</span><span class="p">,</span><span class="n">bpe</span><span class="p">,</span><span class="n">onmt_tokenize</span><span class="p">,</span><span class="n">normalize</span><span class="p">}</span> <span class="p">[{</span><span class="n">insert_mask_before_placeholder</span><span class="p">,</span><span class="n">uppercase</span><span class="p">,</span><span class="n">inlinetags</span><span class="p">,</span><span class="n">bart</span><span class="p">,</span><span class="n">terminology</span><span class="p">,</span><span class="n">docify</span><span class="p">,</span><span class="n">inferfeats</span><span class="p">,</span><span class="n">filtertoolong</span><span class="p">,</span><span class="n">prefix</span><span class="p">,</span><span class="n">suffix</span><span class="p">,</span><span class="n">fuzzymatch</span><span class="p">,</span><span class="n">clean</span><span class="p">,</span><span class="n">switchout</span><span class="p">,</span><span class="n">tokendrop</span><span class="p">,</span><span class="n">tokenmask</span><span class="p">,</span><span class="n">sentencepiece</span><span class="p">,</span><span class="n">bpe</span><span class="p">,</span><span class="n">onmt_tokenize</span><span class="p">,</span><span class="n">normalize</span><span class="p">}</span> <span class="o">...</span><span class="p">]]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">response_patterns</span> <span class="n">RESPONSE_PATTERNS</span> <span class="p">[</span><span class="n">RESPONSE_PATTERNS</span> <span class="o">...</span><span class="p">]]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">upper_corpus_ratio</span> <span class="n">UPPER_CORPUS_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">tags_dictionary_path</span> <span class="n">TAGS_DICTIONARY_PATH</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">tags_corpus_ratio</span> <span class="n">TAGS_CORPUS_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">max_tags</span> <span class="n">MAX_TAGS</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">paired_stag</span> <span class="n">PAIRED_STAG</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">paired_etag</span> <span class="n">PAIRED_ETAG</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">isolated_tag</span> <span class="n">ISOLATED_TAG</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_delimiter</span> <span class="n">SRC_DELIMITER</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">permute_sent_ratio</span> <span class="n">PERMUTE_SENT_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">rotate_ratio</span> <span class="n">ROTATE_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">insert_ratio</span> <span class="n">INSERT_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">random_ratio</span> <span class="n">RANDOM_RATIO</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">mask_ratio</span> <span class="n">MASK_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">mask_length</span> <span class="p">{</span><span class="n">subword</span><span class="p">,</span><span class="n">word</span><span class="p">,</span><span class="n">span</span><span class="o">-</span><span class="n">poisson</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">poisson_lambda</span> <span class="n">POISSON_LAMBDA</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">replace_length</span> <span class="p">{</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">termbase_path</span> <span class="n">TERMBASE_PATH</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_spacy_language_model</span> <span class="n">SRC_SPACY_LANGUAGE_MODEL</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">tgt_spacy_language_model</span> <span class="n">TGT_SPACY_LANGUAGE_MODEL</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">term_corpus_ratio</span> <span class="n">TERM_CORPUS_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">term_example_ratio</span> <span class="n">TERM_EXAMPLE_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_term_stoken</span> <span class="n">SRC_TERM_STOKEN</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">tgt_term_stoken</span> <span class="n">TGT_TERM_STOKEN</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">tgt_term_etoken</span> <span class="n">TGT_TERM_ETOKEN</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">term_source_delimiter</span> <span class="n">TERM_SOURCE_DELIMITER</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">doc_length</span> <span class="n">DOC_LENGTH</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">max_context</span> <span class="n">MAX_CONTEXT</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">reversible_tokenization</span> <span class="p">{</span><span class="n">joiner</span><span class="p">,</span><span class="n">spacer</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_seq_length</span> <span class="n">SRC_SEQ_LENGTH</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">tgt_seq_length</span> <span class="n">TGT_SEQ_LENGTH</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_prefix</span> <span class="n">SRC_PREFIX</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">tgt_prefix</span> <span class="n">TGT_PREFIX</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_suffix</span> <span class="n">SRC_SUFFIX</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">tgt_suffix</span> <span class="n">TGT_SUFFIX</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">tm_path</span> <span class="n">TM_PATH</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">fuzzy_corpus_ratio</span> <span class="n">FUZZY_CORPUS_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">fuzzy_threshold</span> <span class="n">FUZZY_THRESHOLD</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">tm_delimiter</span> <span class="n">TM_DELIMITER</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">fuzzy_token</span> <span class="n">FUZZY_TOKEN</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">fuzzymatch_min_length</span> <span class="n">FUZZYMATCH_MIN_LENGTH</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">fuzzymatch_max_length</span> <span class="n">FUZZYMATCH_MAX_LENGTH</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_eq_tgt</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">same_char</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">same_word</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">scripts_ok</span> <span class="p">[</span><span class="n">SCRIPTS_OK</span> <span class="o">...</span><span class="p">]]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">scripts_nok</span> <span class="p">[</span><span class="n">SCRIPTS_NOK</span> <span class="o">...</span><span class="p">]]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_tgt_ratio</span> <span class="n">SRC_TGT_RATIO</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">avg_tok_min</span> <span class="n">AVG_TOK_MIN</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">avg_tok_max</span> <span class="n">AVG_TOK_MAX</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">langid</span> <span class="p">[</span><span class="n">LANGID</span> <span class="o">...</span><span class="p">]]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">switchout_temperature</span> <span class="n">SWITCHOUT_TEMPERATURE</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tokendrop_temperature</span> <span class="n">TOKENDROP_TEMPERATURE</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tokenmask_temperature</span> <span class="n">TOKENMASK_TEMPERATURE</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">src_subword_model</span> <span class="n">SRC_SUBWORD_MODEL</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tgt_subword_model</span> <span class="n">TGT_SUBWORD_MODEL</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">src_subword_nbest</span> <span class="n">SRC_SUBWORD_NBEST</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tgt_subword_nbest</span> <span class="n">TGT_SUBWORD_NBEST</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">src_subword_alpha</span> <span class="n">SRC_SUBWORD_ALPHA</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tgt_subword_alpha</span> <span class="n">TGT_SUBWORD_ALPHA</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">src_subword_vocab</span> <span class="n">SRC_SUBWORD_VOCAB</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tgt_subword_vocab</span> <span class="n">TGT_SUBWORD_VOCAB</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">src_vocab_threshold</span> <span class="n">SRC_VOCAB_THRESHOLD</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tgt_vocab_threshold</span> <span class="n">TGT_VOCAB_THRESHOLD</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">src_subword_type</span> <span class="p">{</span><span class="n">none</span><span class="p">,</span><span class="n">sentencepiece</span><span class="p">,</span><span class="n">bpe</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tgt_subword_type</span> <span class="p">{</span><span class="n">none</span><span class="p">,</span><span class="n">sentencepiece</span><span class="p">,</span><span class="n">bpe</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">src_onmttok_kwargs</span> <span class="n">SRC_ONMTTOK_KWARGS</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">-</span><span class="n">tgt_onmttok_kwargs</span> <span class="n">TGT_ONMTTOK_KWARGS</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">gpt2_pretok</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">src_lang</span> <span class="n">SRC_LANG</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">tgt_lang</span> <span class="n">TGT_LANG</span><span class="p">]</span> <span class="p">[</span><span class="o">--</span><span class="n">penn</span> <span class="n">PENN</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">norm_quote_commas</span> <span class="n">NORM_QUOTE_COMMAS</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">norm_numbers</span> <span class="n">NORM_NUMBERS</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">pre_replace_unicode_punct</span> <span class="n">PRE_REPLACE_UNICODE_PUNCT</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">post_remove_control_chars</span> <span class="n">POST_REMOVE_CONTROL_CHARS</span><span class="p">]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">quant_layers</span> <span class="n">QUANT_LAYERS</span> <span class="p">[</span><span class="n">QUANT_LAYERS</span> <span class="o">...</span><span class="p">]]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">quant_type</span> <span class="p">{,</span><span class="n">bnb_8bit</span><span class="p">,</span><span class="n">bnb_FP4</span><span class="p">,</span><span class="n">bnb_NF4</span><span class="p">,</span><span class="n">awq_gemm</span><span class="p">,</span><span class="n">awq_gemv</span><span class="p">}]</span>
                    <span class="p">[</span><span class="o">--</span><span class="n">w_bit</span> <span class="p">{</span><span class="mi">4</span><span class="p">}]</span> <span class="p">[</span><span class="o">--</span><span class="n">group_size</span> <span class="p">{</span><span class="mi">128</span><span class="p">}]</span>
</pre></div>
</div>
<section id="Configuration">
<h2>Configuration<a class="headerlink" href="#Configuration" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>-config, --config</kbd></dt>
<dd><p>Path of the main YAML config file.</p>
</dd>
<dt><kbd>-save_config, --save_config</kbd></dt>
<dd><p>Path where to save the config.</p>
</dd>
</dl>
</section>
<section id="Model">
<h2>Model<a class="headerlink" href="#Model" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--model, -model</kbd></dt>
<dd><p>Path to model .pt file(s). Multiple models can be specified, for ensemble decoding.</p>
<p>Default: []</p>
</dd>
<dt><kbd>--precision, -precision</kbd></dt>
<dd><p>Possible choices: , fp32, fp16, int8</p>
<p>Precision to run inference.default is model.dtypefp32 to force slow FP16 model on GTX1080int8 enables pytorch native 8-bit quantization(cpu only)</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--fp32, -fp32</kbd></dt>
<dd><p>Deprecated use ‘precision’ instead</p>
</dd>
<dt><kbd>--int8, -int8</kbd></dt>
<dd><p>Deprecated use ‘precision’ instead</p>
</dd>
<dt><kbd>--avg_raw_probs, -avg_raw_probs</kbd></dt>
<dd><p>If this is set, during ensembling scores from different models will be combined by averaging their raw probabilities and then taking the log. Otherwise, the log probabilities will be averaged directly. Necessary for models whose output layers can assign zero probability.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--self_attn_type, -self_attn_type</kbd></dt>
<dd><p>Self attention type in Transformer decoder layer – currently “scaled-dot”, “scaled-dot-flash” or “average”</p>
<p>Default: “scaled-dot-flash”</p>
</dd>
</dl>
</section>
<section id="Data">
<h2>Data<a class="headerlink" href="#Data" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--data_type, -data_type</kbd></dt>
<dd><p>Type of the source input. Options: [text].</p>
<p>Default: “text”</p>
</dd>
<dt><kbd>--src, -src</kbd></dt>
<dd><p>Source sequence to decode (one line per sequence)</p>
</dd>
<dt><kbd>--tgt, -tgt</kbd></dt>
<dd><p>True target sequence (optional)</p>
</dd>
<dt><kbd>--tgt_file_prefix, -tgt_file_prefix</kbd></dt>
<dd><p>Generate predictions using provided <cite>-tgt</cite> as prefix.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--output, -output</kbd></dt>
<dd><p>Path to output the predictions (each line will be the decoded sequence</p>
<p>Default: “pred.txt”</p>
</dd>
<dt><kbd>--report_align, -report_align</kbd></dt>
<dd><p>Report alignment for each translation.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--gold_align, -gold_align</kbd></dt>
<dd><p>Report alignment between source and gold target.Useful to test the performance of learnt alignments.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--report_time, -report_time</kbd></dt>
<dd><p>Report some translation time metrics</p>
<p>Default: False</p>
</dd>
<dt><kbd>--profile, -profile</kbd></dt>
<dd><p>Report pytorch profiling stats</p>
<p>Default: False</p>
</dd>
</dl>
</section>
<section id="Features">
<h2>Features<a class="headerlink" href="#Features" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>-n_src_feats, --n_src_feats</kbd></dt>
<dd><p>Number of source feats.</p>
<p>Default: 0</p>
</dd>
<dt><kbd>-src_feats_defaults, --src_feats_defaults</kbd></dt>
<dd><p>Default features to apply in source in case there are not annotated</p>
</dd>
</dl>
</section>
<section id="Beam Search">
<h2>Beam Search<a class="headerlink" href="#Beam Search" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--beam_size, -beam_size</kbd></dt>
<dd><p>Beam size</p>
<p>Default: 5</p>
</dd>
<dt><kbd>--ratio, -ratio</kbd></dt>
<dd><p>Ratio based beam stop condition</p>
<p>Default: -0.0</p>
</dd>
</dl>
</section>
<section id="Random Sampling">
<h2>Random Sampling<a class="headerlink" href="#Random Sampling" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--random_sampling_topk, -random_sampling_topk</kbd></dt>
<dd><p>Set this to -1 to do random sampling from full distribution. Set this to value k&gt;1 to do random sampling restricted to the k most likely next tokens. Set this to 1 to use argmax.</p>
<p>Default: 0</p>
</dd>
<dt><kbd>--random_sampling_topp, -random_sampling_topp</kbd></dt>
<dd><p>Probability for top-p/nucleus sampling. Restrict tokens to the most likely until the cumulated probability is over p. In range [0, 1]. <a class="reference external" href="https://arxiv.org/abs/1904.09751">https://arxiv.org/abs/1904.09751</a></p>
<p>Default: 0.0</p>
</dd>
<dt><kbd>--random_sampling_temp, -random_sampling_temp</kbd></dt>
<dd><p>If doing random sampling, divide the logits by this before computing softmax during decoding.</p>
<p>Default: 1.0</p>
</dd>
<dt><kbd>--beam_size, -beam_size</kbd></dt>
<dd><p>Beam size</p>
<p>Default: 5</p>
</dd>
</dl>
</section>
<section id="Reproducibility">
<h2>Reproducibility<a class="headerlink" href="#Reproducibility" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--seed, -seed</kbd></dt>
<dd><p>Set random seed used for better reproducibility between experiments.</p>
<p>Default: -1</p>
</dd>
</dl>
</section>
<section id="Penalties">
<h2>Penalties<a class="headerlink" href="#Penalties" title="Permalink to this heading">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Coverage Penalty is not available in sampling.</p>
</div>
<dl class="option-list">
<dt><kbd>--length_penalty, -length_penalty</kbd></dt>
<dd><p>Possible choices: none, wu, avg</p>
<p>Length Penalty to use.</p>
<p>Default: “avg”</p>
</dd>
<dt><kbd>--alpha, -alpha</kbd></dt>
<dd><p>Length penalty parameter(higher = longer generation)</p>
<p>Default: 1.0</p>
</dd>
<dt><kbd>--coverage_penalty, -coverage_penalty</kbd></dt>
<dd><p>Possible choices: none, wu, summary</p>
<p>Coverage Penalty to use. Only available in beam search.</p>
<p>Default: “none”</p>
</dd>
<dt><kbd>--beta, -beta</kbd></dt>
<dd><p>Coverage penalty parameter</p>
<p>Default: -0.0</p>
</dd>
<dt><kbd>--stepwise_penalty, -stepwise_penalty</kbd></dt>
<dd><p>Apply coverage penalty at every decoding step. Helpful for summary penalty.</p>
<p>Default: False</p>
</dd>
</dl>
</section>
<section id="Decoding tricks">
<h2>Decoding tricks<a class="headerlink" href="#Decoding tricks" title="Permalink to this heading">¶</a></h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Following options can be used to limit the decoding length or content.</p>
</div>
<dl class="option-list">
<dt><kbd>--min_length, -min_length</kbd></dt>
<dd><p>Minimum prediction length</p>
<p>Default: 0</p>
</dd>
<dt><kbd>--max_length, -max_length</kbd></dt>
<dd><p>Maximum prediction length.</p>
<p>Default: 250</p>
</dd>
<dt><kbd>--max_length_ratio, -max_length_ratio</kbd></dt>
<dd><p>Maximum prediction length ratio.for European languages 1.25 is large enoughfor target Asian characters need to increase to 2-3for special languages (burmese, amharic) to 10</p>
<p>Default: 1.25</p>
</dd>
<dt><kbd>--block_ngram_repeat, -block_ngram_repeat</kbd></dt>
<dd><p>Block repetition of ngrams during decoding.</p>
<p>Default: 0</p>
</dd>
<dt><kbd>--ignore_when_blocking, -ignore_when_blocking</kbd></dt>
<dd><p>Ignore these strings when blocking repeats. You want to block sentence delimiters.</p>
<p>Default: []</p>
</dd>
<dt><kbd>--replace_unk, -replace_unk</kbd></dt>
<dd><p>Replace the generated UNK tokens with the source token that had highest attention weight. If phrase_table is provided, it will look up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.</p>
<p>Default: False</p>
</dd>
<dt><kbd>--ban_unk_token, -ban_unk_token</kbd></dt>
<dd><p>Prevent unk token generation by setting unk proba to 0</p>
<p>Default: False</p>
</dd>
<dt><kbd>--phrase_table, -phrase_table</kbd></dt>
<dd><p>If phrase_table is provided (with replace_unk), it will look up the identified source token and give the corresponding target token. If it is not provided (or the identified source token does not exist in the table), then it will copy the source token.</p>
<p>Default: “”</p>
</dd>
</dl>
</section>
<section id="Logging">
<h2>Logging<a class="headerlink" href="#Logging" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--log_file, -log_file</kbd></dt>
<dd><p>Output logs to a file under this path.</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--log_file_level, -log_file_level</kbd></dt>
<dd><p>Possible choices: CRITICAL, ERROR, WARNING, INFO, DEBUG, NOTSET, 50, 40, 30, 20, 10, 0</p>
<p>Default: “0”</p>
</dd>
<dt><kbd>--verbose, -verbose</kbd></dt>
<dd><p>Print scores and predictions for each sentence</p>
<p>Default: False</p>
</dd>
<dt><kbd>--attn_debug, -attn_debug</kbd></dt>
<dd><p>Print best attn for each word</p>
<p>Default: False</p>
</dd>
<dt><kbd>--align_debug, -align_debug</kbd></dt>
<dd><p>Print best align for each word</p>
<p>Default: False</p>
</dd>
<dt><kbd>--dump_beam, -dump_beam</kbd></dt>
<dd><p>File to dump beam information to.</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--n_best, -n_best</kbd></dt>
<dd><p>If verbose is set, will output the n_best decoded sentences</p>
<p>Default: 1</p>
</dd>
<dt><kbd>--with_score, -with_score</kbd></dt>
<dd><p>add a tab separated score to the translation</p>
<p>Default: False</p>
</dd>
</dl>
</section>
<section id="Distributed">
<h2>Distributed<a class="headerlink" href="#Distributed" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--gpu_ranks, -gpu_ranks</kbd></dt>
<dd><p>list of ranks of each process.</p>
<p>Default: []</p>
</dd>
<dt><kbd>--world_size, -world_size</kbd></dt>
<dd><p>total number of distributed processes.</p>
<p>Default: 1</p>
</dd>
<dt><kbd>--parallel_mode, -parallel_mode</kbd></dt>
<dd><p>Possible choices: tensor_parallel, data_parallel</p>
<p>Distributed mode.</p>
<p>Default: “data_parallel”</p>
</dd>
<dt><kbd>--gpu_backend, -gpu_backend</kbd></dt>
<dd><p>Type of torch distributed backend</p>
<p>Default: “nccl”</p>
</dd>
<dt><kbd>--gpu_verbose_level, -gpu_verbose_level</kbd></dt>
<dd><p>Gives more info on each process per GPU.</p>
<p>Default: 0</p>
</dd>
<dt><kbd>--master_ip, -master_ip</kbd></dt>
<dd><p>IP of master for torch.distributed training.</p>
<p>Default: “localhost”</p>
</dd>
<dt><kbd>--master_port, -master_port</kbd></dt>
<dd><p>Port of master for torch.distributed training.</p>
<p>Default: 10000</p>
</dd>
<dt><kbd>--timeout, -timeout</kbd></dt>
<dd><p>Timeout for one GOU to wait for the others.</p>
<p>Default: 60</p>
</dd>
</dl>
</section>
<section id="Efficiency">
<h2>Efficiency<a class="headerlink" href="#Efficiency" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--batch_size, -batch_size</kbd></dt>
<dd><p>Batch size</p>
<p>Default: 30</p>
</dd>
<dt><kbd>--batch_type, -batch_type</kbd></dt>
<dd><p>Possible choices: sents, tokens</p>
<p>Batch grouping for batch_size. Standard is sents. Tokens will do dynamic batching</p>
<p>Default: “sents”</p>
</dd>
<dt><kbd>--gpu, -gpu</kbd></dt>
<dd><p>Device to run on</p>
<p>Default: -1</p>
</dd>
<dt><kbd>-transforms, --transforms</kbd></dt>
<dd><p>Possible choices: insert_mask_before_placeholder, uppercase, inlinetags, bart, terminology, docify, inferfeats, filtertoolong, prefix, suffix, fuzzymatch, clean, switchout, tokendrop, tokenmask, sentencepiece, bpe, onmt_tokenize, normalize</p>
<p>Default transform pipeline to apply to data.</p>
<p>Default: []</p>
</dd>
</dl>
</section>
<section id="Transform/InsertMaskBeforePlaceholdersTransform">
<h2>Transform/InsertMaskBeforePlaceholdersTransform<a class="headerlink" href="#Transform/InsertMaskBeforePlaceholdersTransform" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--response_patterns, -response_patterns</kbd></dt>
<dd><p>Response patten to locate the end of the prompt</p>
<p>Default: [‘Response : ｟newline｠’]</p>
</dd>
</dl>
</section>
<section id="Transform/Uppercase">
<h2>Transform/Uppercase<a class="headerlink" href="#Transform/Uppercase" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--upper_corpus_ratio, -upper_corpus_ratio</kbd></dt>
<dd><p>Corpus ratio to apply uppercasing.</p>
<p>Default: 0.01</p>
</dd>
</dl>
</section>
<section id="Transform/InlineTags">
<h2>Transform/InlineTags<a class="headerlink" href="#Transform/InlineTags" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--tags_dictionary_path, -tags_dictionary_path</kbd></dt>
<dd><p>Path to a flat term dictionary.</p>
</dd>
<dt><kbd>--tags_corpus_ratio, -tags_corpus_ratio</kbd></dt>
<dd><p>Ratio of corpus to augment with tags.</p>
<p>Default: 0.1</p>
</dd>
<dt><kbd>--max_tags, -max_tags</kbd></dt>
<dd><p>Maximum number of tags that can be added to a single sentence.</p>
<p>Default: 12</p>
</dd>
<dt><kbd>--paired_stag, -paired_stag</kbd></dt>
<dd><p>The format of an opening paired inline tag. Must include the character #.</p>
<p>Default: “｟ph_#_beg｠”</p>
</dd>
<dt><kbd>--paired_etag, -paired_etag</kbd></dt>
<dd><p>The format of a closing paired inline tag. Must include the character #.</p>
<p>Default: “｟ph_#_end｠”</p>
</dd>
<dt><kbd>--isolated_tag, -isolated_tag</kbd></dt>
<dd><p>The format of an isolated inline tag. Must include the character #.</p>
<p>Default: “｟ph_#_std｠”</p>
</dd>
<dt><kbd>--src_delimiter, -src_delimiter</kbd></dt>
<dd><p>Any special token used for augmented src sentences. The default is the fuzzy token used in the FuzzyMatch transform.</p>
<p>Default: “｟fuzzy｠”</p>
</dd>
</dl>
</section>
<section id="Transform/BART">
<h2>Transform/BART<a class="headerlink" href="#Transform/BART" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--permute_sent_ratio, -permute_sent_ratio</kbd></dt>
<dd><p>Permute this proportion of sentences (boundaries defined by [‘.’, ‘?’, ‘!’]) in all inputs.</p>
<p>Default: 0.0</p>
</dd>
<dt><kbd>--rotate_ratio, -rotate_ratio</kbd></dt>
<dd><p>Rotate this proportion of inputs.</p>
<p>Default: 0.0</p>
</dd>
<dt><kbd>--insert_ratio, -insert_ratio</kbd></dt>
<dd><p>Insert this percentage of additional random tokens.</p>
<p>Default: 0.0</p>
</dd>
<dt><kbd>--random_ratio, -random_ratio</kbd></dt>
<dd><p>Instead of using &lt;mask&gt;, use random token this often.</p>
<p>Default: 0.0</p>
</dd>
<dt><kbd>--mask_ratio, -mask_ratio</kbd></dt>
<dd><p>Fraction of words/subwords that will be masked.</p>
<p>Default: 0.0</p>
</dd>
<dt><kbd>--mask_length, -mask_length</kbd></dt>
<dd><p>Possible choices: subword, word, span-poisson</p>
<p>Length of masking window to apply.</p>
<p>Default: “subword”</p>
</dd>
<dt><kbd>--poisson_lambda, -poisson_lambda</kbd></dt>
<dd><p>Lambda for Poisson distribution to sample span length if <cite>-mask_length</cite> set to span-poisson.</p>
<p>Default: 3.0</p>
</dd>
<dt><kbd>--replace_length, -replace_length</kbd></dt>
<dd><p>Possible choices: -1, 0, 1</p>
<p>When masking N tokens, replace with 0, 1, or N tokens. (use -1 for N)</p>
<p>Default: -1</p>
</dd>
</dl>
</section>
<section id="Transform/Terminology">
<h2>Transform/Terminology<a class="headerlink" href="#Transform/Terminology" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--termbase_path, -termbase_path</kbd></dt>
<dd><p>Path to a dictionary file with terms.</p>
</dd>
<dt><kbd>--src_spacy_language_model, -src_spacy_language_model</kbd></dt>
<dd><p>Name of the spacy language model for the source corpus.</p>
</dd>
<dt><kbd>--tgt_spacy_language_model, -tgt_spacy_language_model</kbd></dt>
<dd><p>Name of the spacy language model for the target corpus.</p>
</dd>
<dt><kbd>--term_corpus_ratio, -term_corpus_ratio</kbd></dt>
<dd><p>Ratio of corpus to augment with terms.</p>
<p>Default: 0.3</p>
</dd>
<dt><kbd>--term_example_ratio, -term_example_ratio</kbd></dt>
<dd><p>Max terms allowed in an example.</p>
<p>Default: 0.2</p>
</dd>
<dt><kbd>--src_term_stoken, -src_term_stoken</kbd></dt>
<dd><p>The source term start token.</p>
<p>Default: “｟src_term_start｠”</p>
</dd>
<dt><kbd>--tgt_term_stoken, -tgt_term_stoken</kbd></dt>
<dd><p>The target term start token.</p>
<p>Default: “｟tgt_term_start｠”</p>
</dd>
<dt><kbd>--tgt_term_etoken, -tgt_term_etoken</kbd></dt>
<dd><p>The target term end token.</p>
<p>Default: “｟tgt_term_end｠”</p>
</dd>
<dt><kbd>--term_source_delimiter, -term_source_delimiter</kbd></dt>
<dd><p>Any special token used for augmented source sentences. The default is the fuzzy token used in the FuzzyMatch transform.</p>
<p>Default: “｟fuzzy｠”</p>
</dd>
</dl>
</section>
<section id="Transform/Docify">
<h2>Transform/Docify<a class="headerlink" href="#Transform/Docify" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--doc_length, -doc_length</kbd></dt>
<dd><p>Number of tokens per doc.</p>
<p>Default: 200</p>
</dd>
<dt><kbd>--max_context, -max_context</kbd></dt>
<dd><p>Max context segments.</p>
<p>Default: 1</p>
</dd>
</dl>
</section>
<section id="Transform/InferFeats">
<h2>Transform/InferFeats<a class="headerlink" href="#Transform/InferFeats" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--reversible_tokenization, -reversible_tokenization</kbd></dt>
<dd><p>Possible choices: joiner, spacer</p>
<p>Type of reversible tokenization applied on the tokenizer.</p>
<p>Default: “joiner”</p>
</dd>
</dl>
</section>
<section id="Transform/Filter">
<h2>Transform/Filter<a class="headerlink" href="#Transform/Filter" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--src_seq_length, -src_seq_length</kbd></dt>
<dd><p>Maximum source sequence length.</p>
<p>Default: 192</p>
</dd>
<dt><kbd>--tgt_seq_length, -tgt_seq_length</kbd></dt>
<dd><p>Maximum target sequence length.</p>
<p>Default: 192</p>
</dd>
</dl>
</section>
<section id="Transform/Prefix">
<h2>Transform/Prefix<a class="headerlink" href="#Transform/Prefix" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--src_prefix, -src_prefix</kbd></dt>
<dd><p>String to prepend to all source example.</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--tgt_prefix, -tgt_prefix</kbd></dt>
<dd><p>String to prepend to all target example.</p>
<p>Default: “”</p>
</dd>
</dl>
</section>
<section id="Transform/Suffix">
<h2>Transform/Suffix<a class="headerlink" href="#Transform/Suffix" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--src_suffix, -src_suffix</kbd></dt>
<dd><p>String to append to all source example.</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--tgt_suffix, -tgt_suffix</kbd></dt>
<dd><p>String to append to all target example.</p>
<p>Default: “”</p>
</dd>
</dl>
</section>
<section id="Transform/FuzzyMatching">
<h2>Transform/FuzzyMatching<a class="headerlink" href="#Transform/FuzzyMatching" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--tm_path, -tm_path</kbd></dt>
<dd><p>Path to a flat text TM.</p>
</dd>
<dt><kbd>--fuzzy_corpus_ratio, -fuzzy_corpus_ratio</kbd></dt>
<dd><p>Ratio of corpus to augment with fuzzy matches.</p>
<p>Default: 0.1</p>
</dd>
<dt><kbd>--fuzzy_threshold, -fuzzy_threshold</kbd></dt>
<dd><p>The fuzzy matching threshold.</p>
<p>Default: 70</p>
</dd>
<dt><kbd>--tm_delimiter, -tm_delimiter</kbd></dt>
<dd><p>The delimiter used in the flat text TM.</p>
<p>Default: “      “</p>
</dd>
<dt><kbd>--fuzzy_token, -fuzzy_token</kbd></dt>
<dd><p>The fuzzy token to be added with the matches.</p>
<p>Default: “｟fuzzy｠”</p>
</dd>
<dt><kbd>--fuzzymatch_min_length, -fuzzymatch_min_length</kbd></dt>
<dd><p>Min length for TM entries and examples to match.</p>
<p>Default: 4</p>
</dd>
<dt><kbd>--fuzzymatch_max_length, -fuzzymatch_max_length</kbd></dt>
<dd><p>Max length for TM entries and examples to match.</p>
<p>Default: 70</p>
</dd>
</dl>
</section>
<section id="Transform/Clean">
<h2>Transform/Clean<a class="headerlink" href="#Transform/Clean" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--src_eq_tgt, -src_eq_tgt</kbd></dt>
<dd><p>Remove ex src==tgt</p>
<p>Default: False</p>
</dd>
<dt><kbd>--same_char, -same_char</kbd></dt>
<dd><p>Remove ex with same char more than 4 times</p>
<p>Default: False</p>
</dd>
<dt><kbd>--same_word, -same_word</kbd></dt>
<dd><p>Remove ex with same word more than 3 times</p>
<p>Default: False</p>
</dd>
<dt><kbd>--scripts_ok, -scripts_ok</kbd></dt>
<dd><p>list of unicodata scripts accepted</p>
<p>Default: [‘Latin’, ‘Common’]</p>
</dd>
<dt><kbd>--scripts_nok, -scripts_nok</kbd></dt>
<dd><p>list of unicodata scripts not accepted</p>
<p>Default: []</p>
</dd>
<dt><kbd>--src_tgt_ratio, -src_tgt_ratio</kbd></dt>
<dd><p>ratio between src and tgt</p>
<p>Default: 2</p>
</dd>
<dt><kbd>--avg_tok_min, -avg_tok_min</kbd></dt>
<dd><p>average length of tokens min</p>
<p>Default: 3</p>
</dd>
<dt><kbd>--avg_tok_max, -avg_tok_max</kbd></dt>
<dd><p>average length of tokens max</p>
<p>Default: 20</p>
</dd>
<dt><kbd>--langid, -langid</kbd></dt>
<dd><p>list of languages accepted</p>
<p>Default: []</p>
</dd>
</dl>
</section>
<section id="Transform/SwitchOut">
<h2>Transform/SwitchOut<a class="headerlink" href="#Transform/SwitchOut" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>-switchout_temperature, --switchout_temperature</kbd></dt>
<dd><p>Sampling temperature for SwitchOut. <span class="math notranslate nohighlight">\(\tau^{-1}\)</span> in <span id="id1">[<a class="reference internal" href="../ref.html#id40" title="Xinyi Wang, Hieu Pham, Zihang Dai, and Graham Neubig. Switchout: an efficient data augmentation algorithm for neural machine translation. CoRR, 2018. URL: http://arxiv.org/abs/1808.07512, arXiv:1808.07512.">WPDN18</a>]</span>. Smaller value makes data more diverse.</p>
<p>Default: 1.0</p>
</dd>
</dl>
</section>
<section id="Transform/Token_Drop">
<h2>Transform/Token_Drop<a class="headerlink" href="#Transform/Token_Drop" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>-tokendrop_temperature, --tokendrop_temperature</kbd></dt>
<dd><p>Sampling temperature for token deletion.</p>
<p>Default: 1.0</p>
</dd>
</dl>
</section>
<section id="Transform/Token_Mask">
<h2>Transform/Token_Mask<a class="headerlink" href="#Transform/Token_Mask" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>-tokenmask_temperature, --tokenmask_temperature</kbd></dt>
<dd><p>Sampling temperature for token masking.</p>
<p>Default: 1.0</p>
</dd>
</dl>
</section>
<section id="Transform/Subword/Common">
<h2>Transform/Subword/Common<a class="headerlink" href="#Transform/Subword/Common" title="Permalink to this heading">¶</a></h2>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Common options shared by all subword transforms. Including options for indicate subword model path, <a class="reference external" href="https://arxiv.org/abs/1804.10959">Subword Regularization</a>/<a class="reference external" href="https://arxiv.org/abs/1910.13267">BPE-Dropout</a>, and <a class="reference external" href="https://github.com/rsennrich/subword-nmt#best-practice-advice-for-byte-pair-encoding-in-nmt">Vocabulary Restriction</a>.</p>
</div>
<dl class="option-list">
<dt><kbd>-src_subword_model, --src_subword_model</kbd></dt>
<dd><p>Path of subword model for src (or shared).</p>
</dd>
<dt><kbd>-tgt_subword_model, --tgt_subword_model</kbd></dt>
<dd><p>Path of subword model for tgt.</p>
</dd>
<dt><kbd>-src_subword_nbest, --src_subword_nbest</kbd></dt>
<dd><p>Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (source side)</p>
<p>Default: 1</p>
</dd>
<dt><kbd>-tgt_subword_nbest, --tgt_subword_nbest</kbd></dt>
<dd><p>Number of candidates in subword regularization. Valid for unigram sampling, invalid for BPE-dropout. (target side)</p>
<p>Default: 1</p>
</dd>
<dt><kbd>-src_subword_alpha, --src_subword_alpha</kbd></dt>
<dd><p>Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (source side)</p>
<p>Default: 0</p>
</dd>
<dt><kbd>-tgt_subword_alpha, --tgt_subword_alpha</kbd></dt>
<dd><p>Smoothing parameter for sentencepiece unigram sampling, and dropout probability for BPE-dropout. (target side)</p>
<p>Default: 0</p>
</dd>
<dt><kbd>-src_subword_vocab, --src_subword_vocab</kbd></dt>
<dd><p>Path to the vocabulary file for src subword. Format: &lt;word&gt;     &lt;count&gt; per line.</p>
<p>Default: “”</p>
</dd>
<dt><kbd>-tgt_subword_vocab, --tgt_subword_vocab</kbd></dt>
<dd><p>Path to the vocabulary file for tgt subword. Format: &lt;word&gt;     &lt;count&gt; per line.</p>
<p>Default: “”</p>
</dd>
<dt><kbd>-src_vocab_threshold, --src_vocab_threshold</kbd></dt>
<dd><p>Only produce src subword in src_subword_vocab with  frequency &gt;= src_vocab_threshold.</p>
<p>Default: 0</p>
</dd>
<dt><kbd>-tgt_vocab_threshold, --tgt_vocab_threshold</kbd></dt>
<dd><p>Only produce tgt subword in tgt_subword_vocab with  frequency &gt;= tgt_vocab_threshold.</p>
<p>Default: 0</p>
</dd>
</dl>
</section>
<section id="Transform/Subword/ONMTTOK">
<h2>Transform/Subword/ONMTTOK<a class="headerlink" href="#Transform/Subword/ONMTTOK" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>-src_subword_type, --src_subword_type</kbd></dt>
<dd><p>Possible choices: none, sentencepiece, bpe</p>
<p>Type of subword model for src (or shared) in pyonmttok.</p>
<p>Default: “none”</p>
</dd>
<dt><kbd>-tgt_subword_type, --tgt_subword_type</kbd></dt>
<dd><p>Possible choices: none, sentencepiece, bpe</p>
<p>Type of subword model for tgt in  pyonmttok.</p>
<p>Default: “none”</p>
</dd>
<dt><kbd>-src_onmttok_kwargs, --src_onmttok_kwargs</kbd></dt>
<dd><p>Other pyonmttok options for src in dict string, except subword related options listed earlier.</p>
<p>Default: “{‘mode’: ‘none’}”</p>
</dd>
<dt><kbd>-tgt_onmttok_kwargs, --tgt_onmttok_kwargs</kbd></dt>
<dd><p>Other pyonmttok options for tgt in dict string, except subword related options listed earlier.</p>
<p>Default: “{‘mode’: ‘none’}”</p>
</dd>
<dt><kbd>--gpt2_pretok, -gpt2_pretok</kbd></dt>
<dd><p>Preprocess sentence with byte-level mapping</p>
<p>Default: False</p>
</dd>
</dl>
</section>
<section id="Transform/Normalize">
<h2>Transform/Normalize<a class="headerlink" href="#Transform/Normalize" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--src_lang, -src_lang</kbd></dt>
<dd><p>Source language code</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--tgt_lang, -tgt_lang</kbd></dt>
<dd><p>Target language code</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--penn, -penn</kbd></dt>
<dd><p>Penn substitution</p>
<p>Default: True</p>
</dd>
<dt><kbd>--norm_quote_commas, -norm_quote_commas</kbd></dt>
<dd><p>Normalize quotations and commas</p>
<p>Default: True</p>
</dd>
<dt><kbd>--norm_numbers, -norm_numbers</kbd></dt>
<dd><p>Normalize numbers</p>
<p>Default: True</p>
</dd>
<dt><kbd>--pre_replace_unicode_punct, -pre_replace_unicode_punct</kbd></dt>
<dd><p>Replace unicode punct</p>
<p>Default: False</p>
</dd>
<dt><kbd>--post_remove_control_chars, -post_remove_control_chars</kbd></dt>
<dd><p>Remove control chars</p>
<p>Default: False</p>
</dd>
</dl>
</section>
<section id="Quant options">
<h2>Quant options<a class="headerlink" href="#Quant options" title="Permalink to this heading">¶</a></h2>
<dl class="option-list">
<dt><kbd>--quant_layers, -quant_layers</kbd></dt>
<dd><p>list of layers to be compressed in 4/8bit.</p>
<p>Default: []</p>
</dd>
<dt><kbd>--quant_type, -quant_type</kbd></dt>
<dd><p>Possible choices: , bnb_8bit, bnb_FP4, bnb_NF4, awq_gemm, awq_gemv</p>
<p>Type of compression.</p>
<p>Default: “”</p>
</dd>
<dt><kbd>--w_bit, -w_bit</kbd></dt>
<dd><p>Possible choices: 4</p>
<p>W_bit quantization.</p>
<p>Default: 4</p>
</dd>
<dt><kbd>--group_size, -group_size</kbd></dt>
<dd><p>Possible choices: 128</p>
<p>group size quantization.</p>
<p>Default: 128</p>
</dd>
</dl>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="server.html" class="btn btn-neutral float-right" title="Server" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="train.html" class="btn btn-neutral float-left" title="Train" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2023, OpenNMT

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>