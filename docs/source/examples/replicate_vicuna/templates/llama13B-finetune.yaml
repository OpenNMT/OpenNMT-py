# Corpus opts:
data:
    alpaca:
        path_src: "/media/vincent/Extreme SSD/dataAI/alpaca_clean.txt"
        transforms: [sentencepiece, filtertoolong]
        weight: 10
    sharegpt:
        path_src: "/media/vincent/Extreme SSD/dataAI/sharegpt.txt"
        transforms: [sentencepiece, filtertoolong]
        weight: 10

    valid:
        path_src: "/media/vincent/Extreme SSD/dataAI/valid.txt"
        transforms: [sentencepiece]

### Transform related opts:
#### Subword
src_subword_model: "/media/vincent/Extreme\ SSD/dataAI/llama13B/tokenizer.model"
tgt_subword_model: "/media/vincent/Extreme\ SSD/dataAI/llama13B/tokenizer.model"

#### Filter
src_seq_length: 512
tgt_seq_length: 512

# silently ignore empty lines in the data
skip_empty_level: silent

# General opts
train_from: "/media/vincent/Extreme\ SSD/dataAI/llama13B/llama13B-onmt.pt"
save_model: "/media/vincent/Extreme\ SSD/dataAI/llama13B/llama13B-vicuna"
keep_checkpoint: 10
save_checkpoint_steps: 400
seed: 1234
report_every: 10
train_steps: 4000
valid_steps: 400

# Batching
bucket_size: 32768
num_workers: 2
world_size: 1
gpu_ranks: [0]
batch_type: "tokens"
batch_size: 512
valid_batch_size: 256
batch_size_multiple: 1
accum_count: [32]
accum_steps: [0]

override_opts: true  # CAREFULL this requires all settings to be defined below

share_vocab: true
save_data: "/media/vincent/Extreme SSD/dataAI/llama13B"
src_vocab: "/media/vincent/Extreme SSD/dataAI/llama13B/llama.vocab"
src_vocab_size: 32000
tgt_vocab_size: 32000

decoder_start_token: '<s>'
# Optimization
model_dtype: "fp16"
apex_opt_level: ""
optim: "fusedadam"
learning_rate: 0.0002
warmup_steps: 100
decay_method: "none"
adam_beta2: 0.998
max_grad_norm: 0
label_smoothing: 0.0
param_init: 0
param_init_glorot: true
normalization: "tokens"

#4/8bit
quant_layers: ['linear_values', 'linear_query', 'linear_keys', 'final_linear', 'w_1', 'w_2', 'w_3']
quant_type: "bnb_NF4"

#LoRa
lora_layers: ['linear_values', 'linear_query', 'linear_keys', 'final_linear']
lora_rank: 8
lora_dropout: 0.05
lora_alpha: 16
lora_embedding: false

# Chekpointing
#use_ckpting: ['ffn', 'lora']

# Model
model_task: lm
encoder_type: transformer_lm
decoder_type: transformer_lm
layer_norm: rms
pos_ffn_activation_fn: 'silu'
max_relative_positions: -1
position_encoding: False
add_qkvbias: False
add_ffnbias: False
multiquery: false
parallel_residual: False
dec_layers: 40
heads: 40
hidden_size: 5120
word_vec_size: 5120
transformer_ff: 13824
dropout_steps: [0]
dropout: [0.0]
attention_dropout: [0.0]
