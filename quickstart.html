

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="EN" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="EN" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Quickstart &mdash; OpenNMT-py  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="https://unpkg.com/mermaid@9.4.0/dist/mermaid.min.js"></script>
        <script>mermaid.initialize({startOnLoad:true});</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Versions" href="changes.html" />
    <link rel="prev" title="Overview" href="main.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> OpenNMT-py
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="main.html">Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Quickstart</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#how-to-train-a-model-from-scratch">How to train a model from scratch</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-prepare-the-data">Step 1: Prepare the data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-train-the-model">Step 2: Train the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-translate">Step 3: Translate</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-generate-with-a-pretrained-llm">How to generate with a pretrained LLM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#step-1-convert-a-model-from-hugging-face-hub">Step 1: Convert a model from Hugging Face Hub</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-prepare-an-inference-yaml-config-file">Step 2: Prepare an inference.yaml config file</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-generate-text">Step 3: Generate text</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#how-to-finetune-a-pretrained-llm">How to finetune a pretrained LLM</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">Step 1: Convert a model from Hugging Face Hub</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-2-prepare-an-finetune-yaml-config-file">Step 2: Prepare an finetune.yaml config file</a></li>
<li class="toctree-l3"><a class="reference internal" href="#step-3-finetune">Step 3: Finetune</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="changes.html">Versions</a></li>
<li class="toctree-l1"><a class="reference internal" href="CONTRIBUTING.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="ref.html">References</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Frequently Asked Questions</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html">How do I use my v2 models in v3 ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-do-i-train-the-transformer-model">How do I train the Transformer model?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#performance-tips">Performance tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#position-encoding-absolute-vs-relative-vs-rotary-embeddings-vs-alibi">Position encoding: Absolute vs Relative vs Rotary Embeddings vs Alibi</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#do-you-support-multi-gpu">Do you support multi-gpu?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-do-i-use-pretrained-embeddings-e-g-glove">How do I use Pretrained embeddings (e.g. GloVe)?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-can-i-ensemble-models-at-inference">How can I ensemble Models at inference?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-can-i-weight-different-corpora-at-training">How can I weight different corpora at training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#what-special-tokens-does-opennmt-py-use">What special tokens does OpenNMT-py use?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-can-i-apply-on-the-fly-tokenization-and-subword-regularization-when-training">How can I apply on-the-fly tokenization and subword regularization when training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms">What are the readily available on-the-fly data transforms?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-can-i-create-custom-on-the-fly-data-transforms">How can I create custom on-the-fly data transforms?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-to-use-lora-and-8bit-loading-to-finetune-a-big-model">How to use LoRa and 8bit loading to finetune a big model ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-to-use-gradient-checkpointing-when-dealing-with-a-big-model">How to use gradient checkpointing when dealing with a big model ?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#can-i-get-word-alignments-while-translating">Can I get word alignments while translating?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-can-i-update-a-checkpoint-s-vocabulary">How can I update a checkpoint’s vocabulary?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-can-i-use-source-word-features">How can I use source word features?</a></li>
<li class="toctree-l1"><a class="reference internal" href="FAQ.html#how-can-i-set-up-a-translation-server">How can I set up a translation server ?</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Examples</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="examples/wmt17/Translation.html">Translation WMT17 en-de</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/wiki_103/LanguageModelGeneration.html">Language Model Wiki-103</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/summary/Summarization.html">Summarization CNN/DM</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/ggnn/GGNN.html">Gated Graph Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples/replicate_vicuna/ReplicateVicuna.html">Supervised Finetuning of llama 7B to replicate Vicuna</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Scripts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="options/build_vocab.html">Build Vocab</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/translate.html">Translate</a></li>
<li class="toctree-l1"><a class="reference internal" href="options/server.html">Server</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="onmt.html">Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.translation.html">Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.translate.translation_server.html">Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="onmt.inputters.html">Data Loaders</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Legacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="legacy/FAQ.html">FAQ (Legacy version)</a></li>
<li class="toctree-l1"><a class="reference internal" href="legacy/im2text.html">Image to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="legacy/speech2text.html">Speech to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="legacy/vid2text.html">Video to Text</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">OpenNMT-py</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>Quickstart</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/quickstart.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="quickstart">
<h1>Quickstart<a class="headerlink" href="#quickstart" title="Permalink to this heading">¶</a></h1>
<section id="how-to-train-a-model-from-scratch">
<h2>How to train a model from scratch<a class="headerlink" href="#how-to-train-a-model-from-scratch" title="Permalink to this heading">¶</a></h2>
<section id="step-1-prepare-the-data">
<h3>Step 1: Prepare the data<a class="headerlink" href="#step-1-prepare-the-data" title="Permalink to this heading">¶</a></h3>
<p>To get started, we propose to download a toy English-German dataset for machine translation containing 10k tokenized sentences:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>wget<span class="w"> </span>https://s3.amazonaws.com/opennmt-trainingdata/toy-ende.tar.gz
tar<span class="w"> </span>xf<span class="w"> </span>toy-ende.tar.gz
<span class="nb">cd</span><span class="w"> </span>toy-ende
</pre></div>
</div>
<p>The data consists of parallel source (<code class="docutils literal notranslate"><span class="pre">src</span></code>) and target (<code class="docutils literal notranslate"><span class="pre">tgt</span></code>) data containing one sentence per line with tokens separated by a space:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">src-train.txt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tgt-train.txt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">src-val.txt</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tgt-val.txt</span></code></p></li>
</ul>
<p>Validation files are used to evaluate the convergence of the training. It usually contains no more than 5k sentences.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>$ head -n 2 toy-ende/src-train.txt
It is not acceptable that , with the help of the national bureaucracies , Parliament &amp;apos;s legislative prerogative should be made null and void by means of implementing provisions whose content , purpose and extent are not laid down in advance .
Federal Master Trainer and Senior Instructor of the Italian Federation of Aerobic Fitness , Group Fitness , Postural Gym , Stretching and Pilates; from 2004 , he has been collaborating with Antiche Terme as personal Trainer and Instructor of Stretching , Pilates and Postural Gym .
</pre></div>
</div>
<p>We need to build a <strong>YAML configuration file</strong> to specify the data that will be used:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># toy_en_de.yaml</span>

<span class="c1">## Where the samples will be written</span>
<span class="nt">save_data</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/run/example</span>
<span class="c1">## Where the vocab(s) will be written</span>
<span class="nt">src_vocab</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/run/example.vocab.src</span>
<span class="nt">tgt_vocab</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/run/example.vocab.tgt</span>
<span class="c1"># Prevent overwriting existing files in the folder</span>
<span class="nt">overwrite</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">False</span>

<span class="c1"># Corpus opts:</span>
<span class="nt">data</span><span class="p">:</span>
<span class="w">    </span><span class="nt">corpus_1</span><span class="p">:</span>
<span class="w">        </span><span class="nt">path_src</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/src-train.txt</span>
<span class="w">        </span><span class="nt">path_tgt</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/tgt-train.txt</span>
<span class="w">    </span><span class="nt">valid</span><span class="p">:</span>
<span class="w">        </span><span class="nt">path_src</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/src-val.txt</span>
<span class="w">        </span><span class="nt">path_tgt</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/tgt-val.txt</span>
<span class="nn">...</span>
</pre></div>
</div>
<p>From this configuration, we can build the vocab(s), that will be necessary to train the model:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt_build_vocab<span class="w"> </span>-config<span class="w"> </span>toy_en_de.yaml<span class="w"> </span>-n_sample<span class="w"> </span><span class="m">10000</span>
</pre></div>
</div>
<p><strong>Notes</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">-n_sample</span></code> is required here – it represents the number of lines sampled from each corpus to build the vocab.</p></li>
<li><p>This configuration is the simplest possible, without any tokenization or other <em>transforms</em>. See <a class="reference external" href="https://github.com/OpenNMT/OpenNMT-py/tree/master/config">other example configurations</a> for more complex pipelines.</p></li>
</ul>
</section>
<section id="step-2-train-the-model">
<h3>Step 2: Train the model<a class="headerlink" href="#step-2-train-the-model" title="Permalink to this heading">¶</a></h3>
<p>To train a model, we need to <strong>add the following to the YAML configuration file</strong>:</p>
<ul class="simple">
<li><p>the vocabulary path(s) that will be used: can be that generated by onmt_build_vocab;</p></li>
<li><p>training specific parameters.</p></li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># toy_en_de.yaml</span>

<span class="nn">...</span>

<span class="c1"># Vocabulary files that were just created</span>
<span class="nt">src_vocab</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/run/example.vocab.src</span>
<span class="nt">tgt_vocab</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/run/example.vocab.tgt</span>

<span class="c1"># Train on a single GPU</span>
<span class="nt">world_size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">gpu_ranks</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="nv">0</span><span class="p p-Indicator">]</span>

<span class="c1"># Where to save the checkpoints</span>
<span class="nt">save_model</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">toy-ende/run/model</span>
<span class="nt">save_checkpoint_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">500</span>
<span class="nt">train_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1000</span>
<span class="nt">valid_steps</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">500</span>
</pre></div>
</div>
<p>Then you can simply run:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt_train<span class="w"> </span>-config<span class="w"> </span>toy_en_de.yaml
</pre></div>
</div>
<p>This configuration will run the default model, which consists of a 2-layer LSTM with 500 hidden units on both the encoder and decoder. It will run on a single GPU (<code class="docutils literal notranslate"><span class="pre">world_size</span> <span class="pre">1</span></code> &amp; <code class="docutils literal notranslate"><span class="pre">gpu_ranks</span> <span class="pre">[0]</span></code>).</p>
<p>Before the training process actually starts, the <code class="docutils literal notranslate"><span class="pre">*.vocab.pt</span></code> together with <code class="docutils literal notranslate"><span class="pre">*.transforms.pt</span></code> can be dumped to <code class="docutils literal notranslate"><span class="pre">-save_data</span></code> with configurations specified in <code class="docutils literal notranslate"><span class="pre">-config</span></code> yaml file by enabling the <code class="docutils literal notranslate"><span class="pre">-dump_fields</span></code> and <code class="docutils literal notranslate"><span class="pre">-dump_transforms</span></code> flags. It is also possible to generate transformed samples to simplify any potentially required visual inspection. The number of sample lines to dump per corpus is set with the <code class="docutils literal notranslate"><span class="pre">-n_sample</span></code> flag.</p>
<p>For more advanded models and parameters, see <a class="reference external" href="https://github.com/OpenNMT/OpenNMT-py/tree/master/config">other example configurations</a> or the <a class="reference internal" href="FAQ.html"><span class="doc">FAQ</span></a>.</p>
</section>
<section id="step-3-translate">
<h3>Step 3: Translate<a class="headerlink" href="#step-3-translate" title="Permalink to this heading">¶</a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt_translate<span class="w"> </span>-model<span class="w"> </span>toy-ende/run/model_step_1000.pt<span class="w"> </span>-src<span class="w"> </span>toy-ende/src-test.txt<span class="w"> </span>-output<span class="w"> </span>toy-ende/pred_1000.txt<span class="w"> </span>-gpu<span class="w"> </span><span class="m">0</span><span class="w"> </span>-verbose
</pre></div>
</div>
<p>Now you have a model which you can use to predict on new data. We do this by running beam search. This will output predictions into <code class="docutils literal notranslate"><span class="pre">toy-ende/pred_1000.txt</span></code>.</p>
<p><strong>Note</strong>:</p>
<p>The predictions are going to be quite terrible, as the demo dataset is small. Try running on some larger datasets!</p>
<p>For example you can download millions of parallel sentences for <a class="reference external" href="http://www.statmt.org/wmt16/translation-task.html">translation</a> or <a class="reference external" href="https://github.com/harvardnlp/sent-summary">summarization</a>.</p>
</section>
</section>
<section id="how-to-generate-with-a-pretrained-llm">
<h2>How to generate with a pretrained LLM<a class="headerlink" href="#how-to-generate-with-a-pretrained-llm" title="Permalink to this heading">¶</a></h2>
<section id="step-1-convert-a-model-from-hugging-face-hub">
<h3>Step 1: Convert a model from Hugging Face Hub<a class="headerlink" href="#step-1-convert-a-model-from-hugging-face-hub" title="Permalink to this heading">¶</a></h3>
<p>You will find in “tools” a set of converters for models 1) from Hugging Face hub: T5, Falcon, MPT, Openllama, Redpajama, Xgen or 2) the legacy Llama from Meta.</p>
<p>T5 (and variant Flan-T5), Llama and Openllama use Sentencepiece.
The command line to convert a model to OpenNMT-py is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">tools</span><span class="o">/</span><span class="n">convert_openllama</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_dir</span> <span class="o">/</span><span class="n">path_to_HF_model</span> <span class="o">--</span><span class="n">tokenizer_model</span> <span class="o">/</span><span class="n">path_to_tokenizer</span><span class="o">.</span><span class="n">model</span> <span class="o">--</span><span class="n">output</span> <span class="o">/</span><span class="n">path_to_Openllama</span><span class="o">-</span><span class="n">onmt</span><span class="o">.</span><span class="n">pt</span> <span class="o">--</span><span class="nb">format</span> <span class="p">[</span><span class="s1">&#39;pytorch&#39;</span><span class="p">,</span> <span class="s1">&#39;safetensors&#39;</span><span class="p">]</span> <span class="o">--</span><span class="n">nshards</span> <span class="n">N</span>
</pre></div>
</div>
<p>Other models uses BPE, we had to reconstruct the BPE model and vocab file:</p>
<p><a class="reference external" href="https://opennmt-models.s3.amazonaws.com/mosaic-MPT/mpt-model.bpe">MPT bpe model</a></p>
<p><a class="reference external" href="https://opennmt-models.s3.amazonaws.com/mosaic-MPT/mpt.vocab">MPT vocab</a></p>
<p><a class="reference external" href="https://opennmt-models.s3.amazonaws.com/redpajama/redpajama-model.bpe">Redpajama bpe model</a></p>
<p><a class="reference external" href="https://opennmt-models.s3.amazonaws.com/redpajama/redpajama.vocab">Redpajama vocab</a></p>
<p><a class="reference external" href="https://opennmt-models.s3.amazonaws.com/falcon/falcon-model.bpe">Falcon bpe model</a></p>
<p><a class="reference external" href="https://opennmt-models.s3.amazonaws.com/falcon/falcon.vocab">Falcon vocab</a></p>
<p>The command line to convert a model to OpenNMT-py is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">tools</span><span class="o">/</span><span class="n">convert_mpt</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_dir</span> <span class="o">/</span><span class="n">path_to_HF_model</span> <span class="o">--</span><span class="n">vocab_file</span> <span class="o">/</span><span class="n">path_to_mpt</span><span class="o">.</span><span class="n">vocab</span> <span class="o">--</span><span class="n">output</span> <span class="o">/</span><span class="n">path_to_MPT</span><span class="o">-</span><span class="n">onmt</span><span class="o">.</span><span class="n">pt</span> <span class="o">--</span><span class="nb">format</span> <span class="p">[</span><span class="s1">&#39;pytorch&#39;</span><span class="p">,</span> <span class="s1">&#39;safetensors&#39;</span><span class="p">]</span> <span class="o">--</span><span class="n">nshards</span> <span class="n">N</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">/path_to_HF_model</span></code> can be directly a Huggin Face repo.</p>
</section>
<section id="step-2-prepare-an-inference-yaml-config-file">
<h3>Step 2: Prepare an inference.yaml config file<a class="headerlink" href="#step-2-prepare-an-inference-yaml-config-file" title="Permalink to this heading">¶</a></h3>
<p>Even though it is not mandatory, the best way to run inference is to use a config file; here is an example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">transforms</span><span class="p">:</span> <span class="p">[</span><span class="n">sentencepiece</span><span class="p">]</span>

<span class="c1">#### Subword</span>
<span class="n">src_subword_model</span><span class="p">:</span> <span class="s2">&quot;/path_to/llama7B/tokenizer.model&quot;</span>
<span class="n">tgt_subword_model</span><span class="p">:</span> <span class="s2">&quot;/path_to/llama7B/tokenizer.model&quot;</span>

<span class="c1"># Model info</span>
<span class="n">model</span><span class="p">:</span> <span class="s2">&quot;/path_to/llama7B/llama7B-onmt.pt&quot;</span>

<span class="c1"># Inference</span>
<span class="n">seed</span><span class="p">:</span> <span class="mi">42</span>
<span class="n">max_length</span><span class="p">:</span> <span class="mi">256</span>
<span class="n">gpu</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">batch_type</span><span class="p">:</span> <span class="n">sents</span>
<span class="n">batch_size</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">precision</span><span class="p">:</span> <span class="n">fp16</span>
<span class="c1">#random_sampling_topk: 40</span>
<span class="c1">#random_sampling_topp: 0.75</span>
<span class="c1">#random_sampling_temp: 0.1</span>
<span class="n">beam_size</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">n_best</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">report_time</span><span class="p">:</span> <span class="n">true</span>
</pre></div>
</div>
<p>or similarly for a model using BPE:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">transforms</span><span class="p">:</span> <span class="p">[</span><span class="n">onmt_tokenize</span><span class="p">]</span>

<span class="c1">#### Subword</span>
<span class="n">src_subword_type</span><span class="p">:</span> <span class="n">bpe</span>
<span class="n">src_subword_model</span><span class="p">:</span> <span class="s2">&quot;/path_to/mpt7B/mpt-model.bpe&quot;</span>
<span class="n">src_onmttok_kwargs</span><span class="p">:</span> <span class="s1">&#39;{&quot;mode&quot;: &quot;conservative&quot;}&#39;</span>

<span class="n">tgt_subword_type</span><span class="p">:</span> <span class="n">bpe</span>
<span class="n">tgt_subword_model</span><span class="p">:</span> <span class="s2">&quot;/path_to/mpt7B/mpt-model.bpe&quot;</span>
<span class="n">tgt_onmttok_kwargs</span><span class="p">:</span> <span class="s1">&#39;{&quot;mode&quot;: &quot;conservative&quot;}&#39;</span>
<span class="n">gpt2_pretok</span><span class="p">:</span> <span class="n">true</span>
<span class="c1"># Model info</span>
<span class="n">model</span><span class="p">:</span> <span class="s2">&quot;/path_to/mpt7B/mpt-onmt.pt&quot;</span>

<span class="c1"># Inference</span>
<span class="n">seed</span><span class="p">:</span> <span class="mi">42</span>
<span class="n">max_length</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">gpu</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">batch_type</span><span class="p">:</span> <span class="n">sents</span>
<span class="n">batch_size</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">precision</span><span class="p">:</span> <span class="n">fp16</span>
<span class="c1">#random_sampling_topk: 40</span>
<span class="c1">#random_sampling_topp: 0.75</span>
<span class="c1">#random_sampling_temp: 0.8</span>
<span class="n">beam_size</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">report_time</span><span class="p">:</span> <span class="n">true</span>
<span class="n">src</span><span class="p">:</span> <span class="kc">None</span>
<span class="n">tgt</span><span class="p">:</span> <span class="kc">None</span>
</pre></div>
</div>
<p>In this second example, we used <code class="docutils literal notranslate"><span class="pre">max_length:</span> <span class="pre">1</span></code> and <code class="docutils literal notranslate"><span class="pre">src:</span> <span class="pre">None</span></code> <code class="docutils literal notranslate"><span class="pre">tgt:</span> <span class="pre">None</span></code> which is typically the configuration to be used in a scoring script like MMLU where it expects only 1 token as the answer.</p>
<p><strong>WARNING</strong>
For inhomogeneous batches with many examples, the potentially high number of tokens inserted in the shortest examples leads to degraded results when attention layer quantization and flash attention are activated.
In practice, in the inference configuration file, when <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> is greater than 1,
delete ‘linear_values’, ‘linear_query’, ‘linear_keys’, ‘final_linear’ from <code class="docutils literal notranslate"><span class="pre">quant_layers</span></code> and specify <code class="docutils literal notranslate"><span class="pre">self_attn_type:</span> <span class="pre">scaled-dot</span></code>.</p>
<p>You can run this script with the following command line:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">eval_llm</span><span class="o">/</span><span class="n">MMLU</span><span class="o">/</span><span class="n">run_mmlu_opennmt</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config</span> <span class="n">myinference</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
</section>
<section id="step-3-generate-text">
<h3>Step 3: Generate text<a class="headerlink" href="#step-3-generate-text" title="Permalink to this heading">¶</a></h3>
<p>Generating text is also easier with an inference config file (in which you can set max_length or ramdom sampling settings):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">onmt</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">translate</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config</span> <span class="o">/</span><span class="n">path_to_config</span><span class="o">/</span><span class="n">llama7B</span><span class="o">/</span><span class="n">llama</span><span class="o">-</span><span class="n">inference</span><span class="o">.</span><span class="n">yaml</span> <span class="o">--</span><span class="n">src</span> <span class="o">/</span><span class="n">path_to_source</span><span class="o">/</span><span class="nb">input</span><span class="o">.</span><span class="n">txt</span> <span class="o">--</span><span class="n">output</span> <span class="o">/</span><span class="n">path_to_target</span><span class="o">/</span><span class="n">output</span><span class="o">.</span><span class="n">txt</span>
</pre></div>
</div>
</section>
</section>
<section id="how-to-finetune-a-pretrained-llm">
<h2>How to finetune a pretrained LLM<a class="headerlink" href="#how-to-finetune-a-pretrained-llm" title="Permalink to this heading">¶</a></h2>
<section id="id1">
<h3>Step 1: Convert a model from Hugging Face Hub<a class="headerlink" href="#id1" title="Permalink to this heading">¶</a></h3>
<p>See instructions in the previous section.</p>
</section>
<section id="step-2-prepare-an-finetune-yaml-config-file">
<h3>Step 2: Prepare an finetune.yaml config file<a class="headerlink" href="#step-2-prepare-an-finetune-yaml-config-file" title="Permalink to this heading">¶</a></h3>
<p>Finetuning requires the same settings as for training.
Here is an example of finetune.yaml file for Llama</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Corpus opts:</span>
<span class="n">data</span><span class="p">:</span>
    <span class="n">alpaca</span><span class="p">:</span>
        <span class="n">path_src</span><span class="p">:</span> <span class="s2">&quot;/path_to/dataAI/alpaca_clean.txt&quot;</span>
        <span class="n">transforms</span><span class="p">:</span> <span class="p">[</span><span class="n">sentencepiece</span><span class="p">,</span> <span class="n">filtertoolong</span><span class="p">]</span>
        <span class="n">weight</span><span class="p">:</span> <span class="mi">10</span>

    <span class="n">valid</span><span class="p">:</span>
        <span class="n">path_src</span><span class="p">:</span> <span class="s2">&quot;/path_to/dataAI/valid.txt&quot;</span>
        <span class="n">transforms</span><span class="p">:</span> <span class="p">[</span><span class="n">sentencepiece</span><span class="p">]</span>

<span class="c1">### Transform related opts:</span>
<span class="c1">#### Subword</span>
<span class="n">src_subword_model</span><span class="p">:</span> <span class="s2">&quot;/path_to/dataAI/llama7B/tokenizer.model&quot;</span>
<span class="n">tgt_subword_model</span><span class="p">:</span> <span class="s2">&quot;/path_to/dataAI/llama7B/tokenizer.model&quot;</span>

<span class="c1">#### Filter</span>
<span class="n">src_seq_length</span><span class="p">:</span> <span class="mi">1024</span>
<span class="n">tgt_seq_length</span><span class="p">:</span> <span class="mi">1024</span>

<span class="c1">#truncated_decoder: 32</span>

<span class="c1"># silently ignore empty lines in the data</span>
<span class="n">skip_empty_level</span><span class="p">:</span> <span class="n">silent</span>

<span class="c1"># General opts</span>
<span class="n">train_from</span><span class="p">:</span> <span class="s2">&quot;/path_to/dataAI/llama7B/llama7B-onmt.pt&quot;</span>
<span class="n">save_model</span><span class="p">:</span> <span class="s2">&quot;/path_to/dataAI/llama7B/llama7B-alpaca&quot;</span>
<span class="n">save_format</span><span class="p">:</span> <span class="n">safetensors</span>
<span class="n">keep_checkpoint</span><span class="p">:</span> <span class="mi">10</span>
<span class="n">save_checkpoint_steps</span><span class="p">:</span> <span class="mi">1000</span>
<span class="n">seed</span><span class="p">:</span> <span class="mi">1234</span>
<span class="n">report_every</span><span class="p">:</span> <span class="mi">10</span>
<span class="n">train_steps</span><span class="p">:</span> <span class="mi">5000</span>
<span class="n">valid_steps</span><span class="p">:</span> <span class="mi">1000</span>

<span class="c1"># Batching</span>
<span class="n">bucket_size</span><span class="p">:</span> <span class="mi">32768</span>
<span class="n">num_workers</span><span class="p">:</span> <span class="mi">2</span>
<span class="n">world_size</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">gpu_ranks</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_type</span><span class="p">:</span> <span class="s2">&quot;tokens&quot;</span>
<span class="n">batch_size</span><span class="p">:</span> <span class="mi">1024</span>
<span class="n">valid_batch_size</span><span class="p">:</span> <span class="mi">256</span>
<span class="n">batch_size_multiple</span><span class="p">:</span> <span class="mi">1</span>
<span class="n">accum_count</span><span class="p">:</span> <span class="p">[</span><span class="mi">32</span><span class="p">]</span>
<span class="n">accum_steps</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">override_opts</span><span class="p">:</span> <span class="n">true</span>  <span class="c1"># CAREFULL this requires all settings to be defined below</span>

<span class="n">share_vocab</span><span class="p">:</span> <span class="n">true</span>
<span class="n">save_data</span><span class="p">:</span> <span class="s2">&quot;/path_to/dataAI/llama7B&quot;</span>
<span class="n">src_vocab</span><span class="p">:</span> <span class="s2">&quot;/path_to/dataAI/llama7B/llama.vocab&quot;</span>
<span class="n">src_vocab_size</span><span class="p">:</span> <span class="mi">32000</span>
<span class="n">tgt_vocab_size</span><span class="p">:</span> <span class="mi">32000</span>

<span class="n">decoder_start_token</span><span class="p">:</span> <span class="s1">&#39;&lt;s&gt;&#39;</span>
<span class="c1"># Optimization</span>
<span class="n">model_dtype</span><span class="p">:</span> <span class="s2">&quot;fp16&quot;</span>
<span class="n">apex_opt_level</span><span class="p">:</span> <span class="s2">&quot;&quot;</span>
<span class="n">optim</span><span class="p">:</span> <span class="s2">&quot;fusedadam&quot;</span>
<span class="n">learning_rate</span><span class="p">:</span> <span class="mf">0.0001</span>
<span class="n">warmup_steps</span><span class="p">:</span> <span class="mi">100</span>
<span class="n">decay_method</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span>
<span class="c1">#learning_rate_decay: 0.98</span>
<span class="c1">#start_decay_steps: 100</span>
<span class="c1">#decay_steps: 10</span>
<span class="n">adam_beta2</span><span class="p">:</span> <span class="mf">0.998</span>
<span class="n">max_grad_norm</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">label_smoothing</span><span class="p">:</span> <span class="mf">0.0</span>
<span class="n">param_init</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">param_init_glorot</span><span class="p">:</span> <span class="n">true</span>
<span class="n">normalization</span><span class="p">:</span> <span class="s2">&quot;tokens&quot;</span>

<span class="c1">#4/8bit</span>
<span class="n">quant_layers</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;w_1&#39;</span><span class="p">,</span> <span class="s1">&#39;w_2&#39;</span><span class="p">,</span> <span class="s1">&#39;w_3&#39;</span><span class="p">,</span> <span class="s1">&#39;linear_values&#39;</span><span class="p">,</span> <span class="s1">&#39;linear_query&#39;</span><span class="p">,</span> <span class="s1">&#39;linear_keys&#39;</span><span class="p">,</span> <span class="s1">&#39;final_linear&#39;</span><span class="p">]</span>
<span class="n">quant_type</span><span class="p">:</span> <span class="s2">&quot;bnb_NF4&quot;</span>

<span class="c1">#LoRa</span>
<span class="n">lora_layers</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;linear_values&#39;</span><span class="p">,</span> <span class="s1">&#39;linear_query&#39;</span><span class="p">,</span> <span class="s1">&#39;linear_keys&#39;</span><span class="p">,</span> <span class="s1">&#39;final_linear&#39;</span><span class="p">]</span>
<span class="n">lora_rank</span><span class="p">:</span> <span class="mi">8</span>
<span class="n">lora_dropout</span><span class="p">:</span> <span class="mf">0.05</span>
<span class="n">lora_alpha</span><span class="p">:</span> <span class="mi">16</span>
<span class="n">lora_embedding</span><span class="p">:</span> <span class="n">false</span>

<span class="c1"># Chekpointing</span>
<span class="c1">#use_ckpting: [&#39;ffn&#39;, &#39;lora&#39;]</span>

<span class="c1"># Model</span>
<span class="n">model_task</span><span class="p">:</span> <span class="n">lm</span>
<span class="n">encoder_type</span><span class="p">:</span> <span class="n">transformer_lm</span>
<span class="n">decoder_type</span><span class="p">:</span> <span class="n">transformer_lm</span>
<span class="n">layer_norm</span><span class="p">:</span> <span class="n">rms</span>
<span class="n">pos_ffn_activation_fn</span><span class="p">:</span> <span class="s1">&#39;silu&#39;</span>
<span class="n">max_relative_positions</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span>
<span class="n">position_encoding</span><span class="p">:</span> <span class="n">false</span>
<span class="n">add_qkvbias</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">add_ffnbias</span><span class="p">:</span> <span class="kc">False</span>
<span class="n">parallel_residual</span><span class="p">:</span> <span class="n">false</span>
<span class="n">dec_layers</span><span class="p">:</span> <span class="mi">32</span>
<span class="n">heads</span><span class="p">:</span> <span class="mi">32</span>
<span class="n">hidden_size</span><span class="p">:</span> <span class="mi">4096</span>
<span class="n">word_vec_size</span><span class="p">:</span> <span class="mi">4096</span>
<span class="n">transformer_ff</span><span class="p">:</span> <span class="mi">11008</span>
<span class="n">dropout_steps</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">dropout</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span>
<span class="n">attention_dropout</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.0</span><span class="p">]</span>
</pre></div>
</div>
<p>If you want to enable the “zero-out prompt loss” mechanism to ignore the prompt when calculating the loss,
you can add the <code class="docutils literal notranslate"><span class="pre">insert_mask_before_placeholder</span></code> transform as well as the <code class="docutils literal notranslate"><span class="pre">zero_out_prompt_loss</span></code> flag:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">transforms</span><span class="p">:</span> <span class="p">[</span><span class="n">insert_mask_before_placeholder</span><span class="p">,</span> <span class="n">sentencepiece</span><span class="p">,</span> <span class="n">filtertoolong</span><span class="p">]</span>
<span class="n">zero_out_prompt_loss</span><span class="p">:</span> <span class="n">true</span>
</pre></div>
</div>
<p>The default value for the response <code class="docutils literal notranslate"><span class="pre">response_pattern</span></code> used to locate the end of the prompt is “Response : ｟newline｠”, but you can choose another to align it with your training data.</p>
</section>
<section id="step-3-finetune">
<h3>Step 3: Finetune<a class="headerlink" href="#step-3-finetune" title="Permalink to this heading">¶</a></h3>
<p>You can the run the training with the regular train.py command line:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">train</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">config</span> <span class="o">/</span><span class="n">path_to</span><span class="o">/</span><span class="n">finetune</span><span class="o">.</span><span class="n">yaml</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="changes.html" class="btn btn-neutral float-right" title="Versions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="main.html" class="btn btn-neutral float-left" title="Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017-2023, OpenNMT

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>