Affirmation 1| L'estimateur de régression linéaire a la plus petite variance parmi tous les estimateurs sans biais. Affirmation 2| Les coefficients α attribués aux classificateurs assemblés par AdaBoost sont toujours non négatifs.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",D
Affirmation 1| RoBERTa effectue un préapprentissage sur un corpus qui est environ 10x plus grand que le corpus sur lequel BERT a effectué son préapprentissage. Affirmation 2| Les ResNeXts en 2018 utilisaient généralement des fonctions d'activation tanh.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",C
"Affirmation 1| Les machines à vecteurs de support, comme les modèles de régression logistique, donnent une distribution de probabilité sur les étiquettes possibles à partir d'un exemple d'entrée. Affirmation 2| Nous nous attendons à ce que les vecteurs de support restent généralement les mêmes lorsque nous passons d'un noyau linéaire à des noyaux polynomiaux d'ordre supérieur.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
"Un problème d'apprentissage automatique implique quatre attributs et une classe. Les attributs ont chacun 3, 2, 2 et 2 valeurs possibles. La classe a 3 valeurs possibles. Combien y a-t-il d'exemples différents maximums possibles ?",12,24,48,72,D
"À l'horizon 2020, quelle architecture est la plus adaptée à la classification des images à haute résolution ?",réseaux convolutifs,réseaux de graphes,des réseaux entièrement connectés,Réseaux RBF,A
Affirmation 1| La log-vraisemblance des données augmentera toujours au cours des itérations successives de l'algorithme de maximisation de l'espérance. Affirmation 2| L'un des inconvénients de l'apprentissage Q est qu'il ne peut être utilisé que lorsque l'apprenant a une connaissance préalable de la manière dont ses actions affectent son environnement.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
Disons que nous avons calculé le gradient de notre fonction de coût et que nous l'avons stocké dans un vecteur g. Quel est le coût d'une mise à jour de la descente de gradient compte tenu du gradient ?,O(D),O(N),O(ND),O(ND^2),A
"Affirmation 1| Pour une variable aléatoire continue x et sa fonction de distribution de probabilité p(x), 0 ≤ p(x) ≤ 1 pour tout x. Affirmation 2| L'arbre de décision est appris en minimisant le gain d'information.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
Considérez le réseau bayésien donné ci-dessous. Combien de paramètres indépendants sont nécessaires pour ce réseau bayésien H -> U <- P <- W ?,2,4,8,16,C
"Lorsque le nombre d'exemples d'apprentissage est infini, votre modèle formé sur ces données aura.. :",Diminution de la variance,Variance plus élevée,Même écart,Aucune de ces réponses,A
Affirmation 1| L'ensemble de tous les rectangles dans le plan 2D (qui inclut les rectangles non alignés dans l'axe) peut briser un ensemble de 5 points. Affirmation 2| La dimension VC du classificateur k-Proches Voisins lorsque k = 1 est infinie.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Le terme ""modèle"" désigne un modèle qui ne peut ni modéliser les données d'apprentissage, ni se généraliser à de nouvelles données.",bon ajustement,surajustement,sous-adaptation,tout ce qui précède,C
Affirmation 1| Le score F1 peut être particulièrement utile pour les ensembles de données présentant un déséquilibre élevé entre les classes. Affirmation 2| L'aire sous la courbe ROC est l'une des principales mesures utilisées pour évaluer les détecteurs d'anomalies.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Affirmation 1| L'algorithme de rétropropagation apprend un réseau neuronal globalement optimal avec des couches cachées. Affirmation 2| La dimension VC d'une ligne devrait être au plus égale à 2, puisque je peux trouver au moins un cas de 3 points qui ne peuvent être brisés par aucune ligne.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
Une entropie élevée signifie que les partitions dans la classification sont,pur,pas pur,utile,inutile,B
"Affirmation 1 : La normalisation des couches est utilisée dans l'article original de ResNet, et non la normalisation par lots. Affirmation 2 : Les DCGAN utilisent l'auto-attention pour stabiliser l'apprentissage.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
"Lors de la construction d'un modèle de régression linéaire pour un ensemble de données particulier, vous observez que le coefficient de l'une des caractéristiques a une valeur négative relativement élevée. Cela suggère que",Cette caractéristique a un effet important sur le modèle (à conserver).,Cette caractéristique n'a pas d'effet important sur le modèle (doit être ignorée).,Il n'est pas possible de se prononcer sur l'importance de cette caractéristique en l'absence d'informations complémentaires.,Rien ne peut être déterminé.,C
"Pour un réseau neuronal, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre le sous-ajustement (c'est-à-dire un modèle à biais élevé) et le surajustement (c'est-à-dire un modèle à variance élevée) :",Le nombre de nœuds cachés,Le taux d'apprentissage,Le choix initial des poids,L'utilisation d'une unité d'entrée à terme constant,A
"Pour la régression polynomiale, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre l'underfitting et l'overfitting :",Le degré polynomial,Apprentissage des poids par inversion de matrice ou par descente de gradient,Variance supposée du bruit gaussien,L'utilisation d'une unité d'entrée à terme constant,A
"Affirmation 1| À partir de 2020, certains modèles atteignent une précision supérieure à 98 % sur CIFAR-10. Affirmation 2| Les ResNets originaux n'ont pas été optimisés avec l'optimiseur Adam.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
L'algorithme K-means :,La dimension de l'espace des caractéristiques ne doit pas être supérieure au nombre d'échantillons.,A la plus petite valeur de la fonction objective lorsque K = 1,Minimise la variance intra-classe pour un nombre donné de grappes.,Converge vers l'optimum global si et seulement si les moyennes initiales sont choisies comme certains des échantillons eux-mêmes.,C
Affirmation 1| Les réseaux VGGN ont des noyaux convolutifs de largeur et de hauteur plus petites que les noyaux de la première couche d'AlexNet. Affirmation 2| Les procédures d'initialisation des poids en fonction des données ont été introduites avant la normalisation par lots.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Quel est le rang de la matrice suivante ? A = [[1, 1, 1], [1, 1, 1], [1, 1, 1]]",0,1,2,3,B
"Affirmation 1| L'estimation de la densité (à l'aide, par exemple, de l'estimateur de densité à noyau) peut être utilisée pour effectuer une classification. Affirmation 2| La correspondance entre la régression logistique et les Naive Bayes gaussiens (avec des covariances de classe identiques) signifie qu'il existe une correspondance biunivoque entre les paramètres des deux classificateurs.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",C
"Supposons que nous souhaitions effectuer un regroupement sur des données spatiales telles que l'emplacement géométrique des maisons. Nous souhaitons produire des grappes de tailles et de formes différentes. Parmi les méthodes suivantes, laquelle est la plus appropriée ?",Arbres de décision,Regroupement basé sur la densité,Regroupement basé sur un modèle,Regroupement par K-moyennes,B
"Affirmation 1| Dans AdaBoost, les poids des exemples mal classés augmentent du même facteur multiplicatif. Affirmation 2| Dans AdaBoost, l'erreur d'apprentissage pondérée e_t du tième classificateur faible sur les données d'apprentissage avec les poids D_t a tendance à augmenter en fonction de t.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
Les estimations MLE ne sont souvent pas souhaitables pour les raisons suivantes,ils sont partiaux,ils ont une variance élevée,ils ne sont pas des estimateurs cohérents,Aucune de ces réponses,B
"La complexité informatique de la descente de gradient est,",linéaire en D,linéaire en N,polynomial dans D,dépend du nombre d'itérations,C
Le calcul de la moyenne des résultats de plusieurs arbres de décision est utile _.,Augmenter la partialité,Diminuer les préjugés,Augmentation de l'écart,Diminution de la variance,D
Le modèle obtenu par l'application de la régression linéaire sur le sous-ensemble de caractéristiques identifié peut être différent du modèle obtenu à la fin du processus d'identification du sous-ensemble au cours de la phase d'identification.,Sélection du meilleur sous-ensemble,Sélection progressive avant,Sélection par étapes en amont,Toutes les réponses ci-dessus,C
Réseaux neuronaux :,Optimiser une fonction objective convexe,Ne peut être formé qu'avec la descente de gradient stochastique,Possibilité d'utiliser une combinaison de différentes fonctions d'activation,Aucune de ces réponses,C
"Supposons que l'incidence d'une maladie D soit d'environ 5 cas pour 100 personnes (c'est-à-dire P(D) = 0,05). Soit la variable aléatoire booléenne D qui signifie qu'un patient ""est atteint de la maladie D"" et la variable aléatoire booléenne TP qui signifie ""test positif"". On sait que les tests de dépistage de la maladie D sont très précis, en ce sens que la probabilité d'obtenir un résultat positif lorsqu'on est atteint de la maladie est de 0,99, et la probabilité d'obtenir un résultat négatif lorsqu'on n'est pas atteint de la maladie est de 0,97. Quelle est P(TP), la probabilité a priori d'un test positif ?",0.0368,0.473,0.078,Aucune de ces réponses,C
"Affirmation 1| Après avoir été mappé dans l'espace des caractéristiques Q à l'aide d'une fonction noyau à base radiale, le 1-NN utilisant la distance euclidienne non pondérée peut être en mesure d'obtenir une meilleure performance de classification que dans l'espace original (bien que nous ne puissions pas le garantir). Affirmation 2| La dimension VC d'un Perceptron est plus petite que la dimension VC d'un SVM linéaire simple.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
L'inconvénient de la recherche en grille est le suivant,Elle ne peut pas être appliquée aux fonctions non différentiables.,Elle ne peut pas être appliquée à des fonctions non continues.,Il est difficile à mettre en œuvre.,Il fonctionne raisonnablement lentement pour la régression linéaire multiple.,D
Prédire la quantité de pluie dans une région en fonction de divers indices est un problème qui se pose à l'adresse ______.,Apprentissage supervisé,Apprentissage non supervisé,Regroupement,Aucune de ces réponses,A
Laquelle des phrases suivantes est FAUSSE en ce qui concerne la régression ?,Il établit un lien entre les entrées et les sorties.,Il est utilisé pour la prédiction.,Il peut être utilisé pour l'interprétation.,Elle permet de découvrir les relations de cause à effet,D
Laquelle des raisons suivantes est la principale raison de l'élagage d'un arbre de décision ?,Pour économiser du temps de calcul pendant les tests,Pour économiser l'espace nécessaire au stockage de l'arbre de décision,Pour réduire l'erreur de l'ensemble d'apprentissage,Pour éviter un surajustement de l'ensemble d'apprentissage,D
Affirmation 1| L'estimateur de la densité du noyau équivaut à effectuer une régression du noyau avec la valeur Yi = 1/n à chaque point Xi de l'ensemble de données original. Affirmation 2| La profondeur d'un arbre de décision appris peut être supérieure au nombre d'exemples de formation utilisés pour créer l'arbre.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
Supposons que votre modèle soit surajouté. Lequel des éléments suivants n'est PAS un moyen valable d'essayer de réduire l'overfitting ?,Augmenter la quantité de données de formation.,Améliorer l'algorithme d'optimisation utilisé pour la minimisation des erreurs.,Réduire la complexité du modèle.,Réduire le bruit dans les données d'apprentissage.,B
Affirmation 1| La fonction softmax est couramment utilisée dans la régression logistique mutliclasse. Affirmation 2| La température d'une distribution softmax non uniforme affecte son entropie.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
Lequel/lesquels des éléments suivants est/sont vrai(s) en ce qui concerne un SVM ?,"Pour des points de données bidimensionnels, l'hyperplan de séparation appris par un SVM linéaire sera une ligne droite.","En théorie, un SVM à noyau gaussien ne peut pas modéliser un hyperplan de séparation complexe.","Pour chaque fonction noyau utilisée dans un SVM, on peut obtenir une expansion de base équivalente sous forme fermée.",Le surajustement d'un SVM n'est pas fonction du nombre de vecteurs de support.,A
"Laquelle des probabilités suivantes est la probabilité conjointe de H, U, P et W décrite par le réseau bayésien H -> U <- P <- W ? [note : comme le produit des probabilités conditionnelles].","P(H, U, P, W) = P(H) * P(W) * P(P) * P(U)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(W | H, P)","P(H, U, P, W) = P(H) * P(W) * P(P | W) * P(U | H, P)",Aucune de ces réponses,C
"Affirmation 1| Puisque la dimension VC d'un SVM avec un noyau à base radiale est infinie, un tel SVM doit être moins bon qu'un SVM avec un noyau polynomial qui a une dimension VC finie. Affirmation 2| Un réseau neuronal à deux couches avec des fonctions d'activation linéaires est essentiellement une combinaison pondérée de séparateurs linéaires, formés sur un ensemble de données donné ; l'algorithme de boosting construit sur des séparateurs linéaires trouve également une combinaison de séparateurs linéaires, et ces deux algorithmes donneront donc le même résultat.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
Affirmation 1| L'algorithme ID3 est garanti de trouver l'arbre de décision optimal. Affirmation 2| Considérons une distribution de probabilité continue avec une densité f() qui est non nulle partout. La probabilité d'une valeur x est égale à f(x).,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
"Étant donné un réseau neuronal à N nœuds d'entrée, sans couches cachées, avec un nœud de sortie, une perte d'entropie et des fonctions d'activation sigmoïde, lequel des algorithmes suivants (avec les hyperparamètres et l'initialisation appropriés) peut être utilisé pour trouver l'optimum global ?",Descente stochastique de gradient,Descente de gradient par mini-lots,Descente de gradient par lots,Toutes les réponses ci-dessus,D
"En ajoutant des fonctions de base supplémentaires dans un modèle linéaire, choisissez l'option la plus probable :",Diminue le biais du modèle,Diminue le biais d'estimation,Diminution de la variance,N'affecte pas le biais et la variance,A
Considérons le réseau bayésien donné ci-dessous. De combien de paramètres indépendants aurions-nous besoin si nous ne faisions aucune hypothèse sur l'indépendance ou l'indépendance conditionnelle H -> U <- P <- W ?,3,4,7,15,D
Un autre terme pour désigner la détection de l'absence de distribution est ?,détection des anomalies,détection à classe unique,robustesse de l'inadéquation formation-test,détection de l'arrière-plan,A
"Affirmation 1| Nous apprenons un classificateur f en renforçant les apprenants faibles h. La forme fonctionnelle de la frontière de décision de f est la même que celle de h, mais avec des paramètres différents (par exemple, si h était un classificateur linéaire, f est également un classificateur linéaire). (par exemple, si h est un classificateur linéaire, f est également un classificateur linéaire). Affirmation 2| La validation croisée peut être utilisée pour sélectionner le nombre d'itérations dans le boosting ; cette procédure peut contribuer à réduire le surajustement.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",D
Affirmation 1| Les réseaux autoroutiers ont été introduits après les ResNets et évitent la mise en commun maximale en faveur des convolutions. Affirmation 2| Les DenseNets coûtent généralement plus de mémoire que les ResNets.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",D
"Si N est le nombre d'instances dans l'ensemble de données d'apprentissage, les plus proches voisins ont un temps d'exécution de la classification de",O(1),O( N ),O(log N ),O( N^2 ),B
"Affirmation 1| Les ResNets et Transformers originaux sont des réseaux neuronaux feedforward. Affirmation 2| Les Transformers originaux utilisent l'auto-attention, ce qui n'est pas le cas du ResNet original.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Affirmation 1| Les RELU ne sont pas monotones, mais les sigmoïdes le sont. Affirmation 2| Les réseaux neuronaux formés par descente de gradient convergent avec une forte probabilité vers l'optimum global.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",D
La sortie numérique d'un nœud sigmoïde dans un réseau neuronal :,est non borné et englobe tous les nombres réels.,"Est non borné, englobant tous les nombres entiers.",Est limité entre 0 et 1.,Est limité entre -1 et 1.,C
Lequel des éléments suivants ne peut être utilisé que lorsque les données d'apprentissage sont linéairement séparables ?,SVM linéaire à marge dure.,Régression logistique linéaire.,SVM à marge douce linéaire.,La méthode des centroïdes.,A
Lesquels des algorithmes suivants sont des algorithmes de regroupement spatial ?,Regroupement basé sur le partitionnement,Regroupement par K-moyennes,Regroupement basé sur une grille,Toutes les réponses ci-dessus,D
Affirmation 1| Les limites de décision à marge maximale que les machines à vecteurs de support construisent ont l'erreur de généralisation la plus faible parmi tous les classificateurs linéaires. Affirmation 2| Toute limite de décision obtenue à partir d'un modèle génératif avec des distributions gaussiennes conditionnelles de classe pourrait en principe être reproduite avec un SVM et un noyau polynomial de degré inférieur ou égal à trois.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",D
Affirmation 1| La régularisation L2 des modèles linéaires tend à rendre les modèles plus clairsemés que la régularisation L1. Affirmation 2| Les connexions résiduelles peuvent être trouvées dans les ResNets et les Transformers.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",D
"Supposons que nous souhaitions calculer P(H|E, F) et que nous ne disposions d'aucune information sur l'indépendance conditionnelle. Parmi les ensembles de nombres suivants, lesquels sont suffisants pour le calcul ?","P(E, F), P(H), P(E|H), P(F|H)","P(E, F), P(H), P(E, F|H)","P(H), P(E|H), P(F|H)","P(E, F), P(E|H), P(F|H)",B
"Parmi les éléments suivants, lequel permet d'éviter l'ajustement excessif lorsque nous procédons à la mise en sac ?",L'utilisation de l'échantillonnage avec remplacement comme technique d'échantillonnage,L'utilisation de classificateurs faibles,L'utilisation d'algorithmes de classification qui ne sont pas sujets à l'ajustement excessif.,La pratique de la validation effectuée sur chaque classificateur formé,B
"Statement 1| PCA et Spectral Clustering (tel que celui d'Andrew Ng) effectuent une décomposition numérique sur deux matrices différentes. Cependant, la taille de ces deux matrices est la même. Affirmation 2| La classification étant un cas particulier de régression, la régression logistique est un cas particulier de régression linéaire.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
"Affirmation 1| La Stanford Sentiment Treebank contenait des critiques de films, pas de livres. Affirmation 2| La Penn Treebank a été utilisée pour la modélisation linguistique.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Quelle est la dimensionnalité de l'espace nul de la matrice suivante ? A = [[3, 2, -9], [-6, -4, 18], [12, 8, -36]]",0,1,2,3,C
Qu'est-ce qu'un vecteur de soutien ?,Les exemples les plus éloignés de la limite de décision.,Les seuls exemples nécessaires pour calculer f(x) dans un SVM.,Le centroïde des données.,Tous les exemples qui ont un poids αk non nul dans un SVM.,B
Instruction 1| Les paramètres de Word2Vec n'ont pas été initialisés à l'aide d'une machine de Boltzman restreinte. Déclaration 2| La fonction tanh est une fonction d'activation non linéaire.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Si la perte d'apprentissage augmente avec le nombre d'époques, lequel des éléments suivants pourrait être un problème dans le processus d'apprentissage ?",La régularisation est trop faible et le modèle est surajusté,La régularisation est trop élevée et le modèle est sous-adapté,La taille du pas est trop grande,La taille du pas est trop petite,C
"Supposons que l'incidence d'une maladie D soit d'environ 5 cas pour 100 personnes (c'est-à-dire P(D) = 0,05). Soit la variable aléatoire booléenne D qui signifie qu'un patient ""est atteint de la maladie D"" et la variable aléatoire booléenne TP qui signifie ""test positif"". On sait que les tests de dépistage de la maladie D sont très précis, en ce sens que la probabilité d'obtenir un résultat positif lorsqu'on est atteint de la maladie est de 0,99, et la probabilité d'obtenir un résultat négatif lorsqu'on n'est pas atteint de la maladie est de 0,97. Quelle est P(D | TP), la probabilité a posteriori que vous ayez la maladie D lorsque le test est positif ?",0.0495,0.078,0.635,0.97,C
"Affirmation 1| Les résultats traditionnels de l'apprentissage automatique supposent que les ensembles de formation et de test sont indépendants et identiquement distribués. Affirmation 2| En 2017, les modèles COCO ont généralement été pré-entraînés sur ImageNet.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Affirmation 1| Les valeurs des marges obtenues par deux noyaux différents K1(x, x0) et K2(x, x0) sur le même ensemble d'apprentissage ne nous indiquent pas quel classificateur sera le plus performant sur l'ensemble de test. Affirmation 2| La fonction d'activation de BERT est le GELU.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
Lequel des algorithmes suivants est un algorithme de regroupement dans l'apprentissage automatique ?,Maximisation des attentes,CART,Naïve Bayes gaussienne,Apriori,A
"Vous venez de terminer l'apprentissage d'un arbre de décision pour la classification des spams, et ses performances sont anormalement mauvaises, tant sur les ensembles d'apprentissage que sur les ensembles de test. Vous savez que votre implémentation n'a pas de bogues, alors quelle pourrait être la cause du problème ?",Vos arbres de décision sont trop superficiels.,Il faut augmenter le taux d'apprentissage.,Vous êtes en train de faire de l'overfitting.,Aucune de ces réponses.,A
La validation croisée K-fold est,linéaire en K,quadratique dans K,cubique en K,exponentiel en K,A
"Affirmation 1| Les réseaux neuronaux à l'échelle industrielle sont normalement formés sur des CPU, et non sur des GPU. Affirmation 2| Le modèle ResNet-50 compte plus d'un milliard de paramètres.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
"Étant donné deux variables aléatoires booléennes, A et B, où P(A) = 1/2, P(B) = 1/3, et P(A | ¬B) = 1/4, quelle est la valeur de P(A | B) ?",1/6,1/4,3/4,1,D
Les risques existentiels posés par l'IA sont le plus souvent associés à l'un des professeurs suivants ?,Nando de Frietas,Yann LeCun,Stuart Russell,Jitendra Malik,C
Affirmation 1| La maximisation de la vraisemblance d'un modèle de régression logistique produit de multiples optimums locaux. Affirmation 2| Aucun classificateur ne peut faire mieux qu'un classificateur de Bayes naïf si la distribution des données est connue.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
"Pour la régression par noyau, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre l'ajustement insuffisant et l'ajustement excessif :","Si la fonction du noyau est gaussienne, triangulaire ou en forme de boîte",Si nous utilisons des métriques euclidiennes ou L1 ou L∞.,La largeur du noyau,Hauteur maximale de la fonction noyau,C
"Affirmation 1| L'algorithme d'apprentissage SVM est assuré de trouver l'hypothèse globalement optimale par rapport à sa fonction objet. Affirmation 2| Après avoir été mappé dans l'espace des caractéristiques Q à l'aide d'une fonction noyau à base radiale, un perceptron peut être en mesure d'obtenir de meilleures performances de classification que dans son espace d'origine (bien que nous ne puissions pas le garantir).","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Pour un classificateur de Bayes gaussien, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre l'underfitting et l'overfitting :",Apprentissage des centres de classe par Maximum de Vraisemblance ou par Descente de Gradient,Que l'on suppose des matrices de covariance de classe complètes ou des matrices de covariance de classe diagonales,Si nous avons des prieurs de classes égales ou des prieurs estimés à partir des données.,Que nous permettions aux classes d'avoir des vecteurs moyens différents ou que nous les obligions à partager le même vecteur moyen,B
Affirmation 1| Le surajustement est plus probable lorsque l'ensemble des données d'apprentissage est petit. Affirmation 2| Le surajustement est plus probable lorsque l'espace d'hypothèses est restreint.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",D
"Affirmation 1| Outre EM, la descente de gradient peut être utilisée pour effectuer l'inférence ou l'apprentissage sur un modèle de mélange gaussien. Affirmation 2 : En supposant un nombre fixe d'attributs, un classificateur optimal de Bayes à base gaussienne peut être appris en un temps linéaire par rapport au nombre d'enregistrements dans l'ensemble de données.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Affirmation 1| Dans un réseau bayésien, les résultats de l'inférence de l'algorithme de l'arbre de jonction sont les mêmes que les résultats de l'inférence de l'élimination des variables. Affirmation 2| Si deux variables aléatoires X et Y sont conditionnellement indépendantes compte tenu d'une autre variable aléatoire Z, alors dans le réseau bayésien correspondant, les nœuds pour X et Y sont d-séparés compte tenu de Z.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",C
"À partir d'un vaste ensemble de dossiers médicaux de patients souffrant de maladies cardiaques, essayez d'apprendre s'il existe différents groupes de patients pour lesquels nous pourrions adapter des traitements distincts. De quel type de problème d'apprentissage s'agit-il ?",Apprentissage supervisé,Apprentissage non supervisé,Les deux points (a) et (b),Ni (a) ni (b),B
Que feriez-vous dans l'ACP pour obtenir la même projection que la SVD ?,Transformer les données en moyenne zéro,Transformer les données en médiane zéro,Impossible,Aucun de ces éléments,A
"Affirmation 1| L'erreur d'apprentissage du classificateur du plus proche voisin est de 0. Affirmation 2| Lorsque le nombre de points de données augmente jusqu'à l'infini, l'estimation MAP se rapproche de l'estimation MLE pour tous les antécédents possibles. En d'autres termes, lorsque le nombre de données est suffisant, le choix de l'a priori n'est pas pertinent.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",C
"Lors d'une régression par moindres carrés avec régularisation (en supposant que l'optimisation puisse être effectuée de manière exacte), l'augmentation de la valeur du paramètre de régularisation λ l'erreur de test.",ne diminuera jamais l'erreur d'apprentissage.,n'augmentera jamais l'erreur d'apprentissage.,ne diminuera jamais l'erreur de test.,n'augmentera jamais,A
Lequel des énoncés suivants décrit le mieux ce que les approches discriminantes tentent de modéliser ? (w sont les paramètres du modèle),"p(y|x, w)","p(y, x)","p(w|x, w)",Aucune de ces réponses,A
Affirmation 1| La performance de classification CIFAR-10 pour les réseaux neuronaux à convolution peut dépasser 95 %. Affirmation 2| Les ensembles de réseaux neuronaux n'améliorent pas la précision de la classification car les représentations qu'ils apprennent sont fortement corrélées.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",C
Sur lequel des points suivants les bayésiens et les fréquentistes sont-ils en désaccord ?,L'utilisation d'un modèle de bruit non gaussien dans la régression probabiliste.,L'utilisation de la modélisation probabiliste pour la régression.,L'utilisation de distributions préalables sur les paramètres d'un modèle probabiliste.,L'utilisation de prieurs de classe dans l'analyse discriminante gaussienne.,C
"Affirmation 1| La métrique BLEU utilise la précision, alors que la métrique ROGUE utilise le rappel. Affirmation 2| Les modèles de markov cachés ont été fréquemment utilisés pour modéliser les phrases anglaises.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
Affirmation 1| ImageNet contient des images de différentes résolutions. Affirmation 2| Caltech-101 a plus d'images qu'ImageNet.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",C
Lequel des éléments suivants est le plus approprié pour effectuer la sélection des caractéristiques ?,Crête,Lasso,à la fois a) et b),ni (a) ni (b),B
Supposons que l'on vous donne un algorithme EM qui trouve des estimations du maximum de vraisemblance pour un modèle avec des variables latentes. On vous demande de modifier l'algorithme pour qu'il trouve des estimations MAP à la place. Quelle(s) étape(s) devez-vous modifier ?,Attentes,Maximisation,Aucune modification n'est nécessaire,Les deux,B
"Pour un classificateur de Bayes gaussien, laquelle de ces hypothèses structurelles est celle qui affecte le plus le compromis entre l'underfitting et l'overfitting :",Apprentissage des centres de classe par Maximum de Vraisemblance ou par Descente de Gradient,Que l'on suppose des matrices de covariance de classe complètes ou des matrices de covariance de classe diagonales,Si nous avons des prieurs de classes égales ou des prieurs estimés à partir des données,Que nous permettions aux classes d'avoir des vecteurs moyens différents ou que nous les obligions à partager le même vecteur moyen,B
"Affirmation 1| Pour deux variables x et y ayant une distribution conjointe p(x, y), on a toujours H[x, y] ≥ H[x] + H[y] où H est la fonction d'entropie. Affirmation 2| Pour certains graphes dirigés, la moralisation diminue le nombre d'arêtes présentes dans le graphe.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",B
Lequel des éléments suivants n'est PAS un apprentissage supervisé ?,APC,Arbre de décision,Régression linéaire,Bayésien naïf,A
Affirmation 1| La convergence d'un réseau neuronal dépend du taux d'apprentissage. Affirmation 2| Dropout multiplie par zéro les valeurs d'activation choisies au hasard.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Lequel des énoncés suivants est égal à P(A, B, C) étant donné les variables aléatoires booléennes A, B et C, et aucune hypothèse d'indépendance ou d'indépendance conditionnelle entre l'une d'entre elles ?",P(A | B) * P(B | C) * P(C | A),"P(C | A, B) * P(A) * P(B)","P(A, B | C) * P(C)","P(A | B, C) * P(B | A, C) * P(C | A, B)",C
"Parmi les tâches suivantes, quelle est celle qui peut être le mieux résolue à l'aide de la technique du regroupement (clustering).",Prévision de la quantité de pluie en fonction de différents indices,Détection des transactions frauduleuses par carte de crédit,Entraîner un robot à résoudre un labyrinthe,Toutes les réponses ci-dessus,B
"Après avoir appliqué une pénalité de régularisation dans une régression linéaire, vous constatez que certains des coefficients de w sont réduits à zéro. Laquelle des pénalités suivantes aurait pu être utilisée ?",Norme L0,Norme L1,Norme L2,"soit a), soit b)",D
"A et B sont deux événements. Si P(A, B) diminue alors que P(A) augmente, laquelle des affirmations suivantes est vraie ?",P(A|B) diminue,P(B|A) diminue,P(B) diminue,Tout ce qui précède,B
"Affirmation 1| Lors de l'apprentissage d'un HMM pour un ensemble fixe d'observations, en supposant que nous ne connaissions pas le nombre réel d'états cachés (ce qui est souvent le cas), nous pouvons toujours augmenter la vraisemblance des données d'apprentissage en autorisant davantage d'états cachés. Affirmation 2| Le filtrage collaboratif est souvent un modèle utile pour modéliser les préférences des utilisateurs en matière de films.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
"Vous entraînez un modèle de régression linéaire pour une tâche d'estimation simple, et vous remarquez que le modèle s'ajuste trop aux données. Vous décidez d'ajouter la régularisation $\ell_2$ pour pénaliser les poids. Lorsque vous augmentez le coefficient de régularisation $\ell_2$, que se passe-t-il au niveau du biais et de la variance du modèle ?",Augmentation du biais ; Augmentation de la variance,Augmentation du biais ; diminution de la variance,Diminution du biais ; augmentation de la variance,Diminution du biais ; Diminution de la variance,B
"Quelle(s) commande(s) PyTorch 1.8 produit $10\times 5$ une matrice gaussienne avec chaque entrée i.i.d. échantillonnée à partir de $\mathcal{N}(\mu=5,\sigma^2=16)$ et $10\times 10$ une matrice uniforme avec chaque entrée i.i.d. échantillonnée à partir de $U[-1,1)$ ?","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{torch.rand(10,10,low=-1,high=1)}","\texttt{5 + torch.randn(10,5) * 16} ; \texttt{(torch.rand(10,10) - 0.5) / 0.5}","\texttt{5 + torch.randn(10,5) * 4} ; \texttt{2 * torch.rand(10,10) - 1}","\texttt{torch.normal(torch.ones(10,5)*5,torch.ones(5,5)*16)} ; \texttt{2 * torch.rand(10,10) - 1}",C
"Affirmation 1| Le gradient de la ReLU est nul pour $x<0$, et le gradient de la sigmoïde $\sigma(x)(1-\sigma(x))\le \frac{1}{4}$ pour tout $x$. Affirmation 2| La sigmoïde a un gradient continu et la ReLU un gradient discontinu.","Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",A
Qu'est-ce qui est vrai à propos de la normalisation par lots ?,"Après l'application de la normalisation par lots, les activations de la couche suivront une distribution gaussienne standard.",Le paramètre de biais des couches affines devient redondant si une couche de normalisation par lots suit immédiatement.,L'initialisation du poids standard doit être modifiée lors de l'utilisation de la normalisation par lots.,La normalisation par lots est équivalente à la normalisation par couches pour les réseaux neuronaux convolutifs.,B
Supposons que nous ayons la fonction objective suivante : $\argmin_{w} \frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\gamma \norm{w}^2_2$ Quel est le gradient de $\frac{1}{2} \norm{Xw-y}^2_2 + \frac{1}{2}\lambda \norm{w}^2_2$ par rapport à $w$ ?,$\nabla_w f(w) = (X^\top X + \lambda I)w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + \nambda$,$\nabla_w f(w) = X^\top X w - X^\top y + \lambda w$,$\nabla_w f(w) = X^\top X w - X^\top y + (\lambda+1) w$,C
Lequel des énoncés suivants s'applique à un noyau de convolution ?,La convolution d'une image avec $\begin{bmatrix}1 & 0 & 0\\ 0 & 1 & 0 \\N 0 & 0 & 1 \Nend{bmatrix}$ ne changerait pas l'image.,La convolution d'une image avec $\begin{bmatrix}0 & 0 & 0\\ 0 & 1 & 0 \\N0 & 0 & 0 \Nend{bmatrix}$ ne changerait pas l'image.,La convolution d'une image avec $\begin{bmatrix}1 & 1 & 1\\ 1 & 1 & 1 \\N1 & 1 & 1 \Nend{bmatrix}$ ne changerait pas l'image.,La convolution d'une image avec $\begin{bmatrix}0 & 0 & 0\\ 0 & 0 & 0 \\N0 & 0 & 0 \Nend{bmatrix}$ ne changerait pas l'image.,B
Lequel des éléments suivants est faux ?,"Les modèles de segmentation sémantique prédisent la classe de chaque pixel, tandis que les classificateurs d'images multiclasses prédisent la classe de l'image entière.",Une boîte englobante dont l'intersection sur l'union (IoU) est égale à 96 $ serait probablement considérée comme un vrai positif.,"Lorsqu'une boîte de délimitation prédite ne correspond à aucun objet de la scène, elle est considérée comme un faux positif.",Une boîte englobante dont l'IoU (intersection sur union) est égale à $3\%$ serait probablement considérée comme un faux négatif.,D
Lequel des éléments suivants est faux ?,"Le réseau entièrement connecté suivant sans fonctions d'activation est linéaire : $g_3(g_2(g_1(x)))$, où $g_i(x) = W_i x$ et $W_i$ sont des matrices.","Le Leaky ReLU $\max\{0.01x,x\}$ est convexe.",Une combinaison de ReLU telle que $ReLU(x) - ReLU(x-1)$ est convexe.,La perte $\log \sigma(x)= -\log(1+e^{-x})$ est concave,C
"Nous formons un réseau entièrement connecté avec deux couches cachées pour prédire les prix des logements. Les données d'entrée ont une dimension de 100 $ et comportent plusieurs caractéristiques telles que le nombre de pieds carrés, le revenu familial médian, etc. La première couche cachée a des activations de $1000$. La deuxième couche cachée a des activations de $10$. La sortie est un scalaire représentant le prix de la maison. En supposant un réseau vanille avec des transformations affines, sans normalisation des lots et sans paramètres apprenables dans la fonction d'activation, combien de paramètres ce réseau possède-t-il ?",111021,110010,111110,110011,A
Affirmation 1| La dérivée de la sigmoïde $\sigma(x)=(1+e^{-x})^{-1}$ par rapport à $x$ est égale à $\text{Var}(B)$ où $B\sim \text{Bern}(\sigma(x))$ est une variable aléatoire de Bernoulli. Affirmation 2| La fixation à 0 des paramètres de biais dans chaque couche du réseau neuronal modifie le compromis biais-variance de sorte que la variance du modèle augmente et que le biais du modèle diminue.,"Vrai, Vrai","Faux, faux","Vrai, Faux","Faux, Vrai",C
