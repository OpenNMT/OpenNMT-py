

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Summarization &mdash; OpenNMT-py  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/theme_overrides.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gated Graph Sequence Neural Networks" href="GGNN.html" />
    <link rel="prev" title="Translation" href="Translation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home"> OpenNMT-py
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../main.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CONTRIBUTING.html">Contributors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../ref.html">References</a></li>
</ul>
<p class="caption"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html">How do I use Pretrained embeddings (e.g. GloVe)?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-do-i-use-the-transformer-model">How do I use the Transformer model?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#do-you-support-multi-gpu">Do you support multi-gpu?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-ensemble-models-at-inference">How can I ensemble Models at inference?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-weight-different-corpora-at-training">How can I weight different corpora at training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-apply-on-the-fly-tokenization-and-subword-regularization-when-training">How can I apply on-the-fly tokenization and subword regularization when training?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#what-are-the-readily-available-on-the-fly-data-transforms">What are the readily available on-the-fly data transforms?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#how-can-i-create-custom-on-the-fly-data-transforms">How can I create custom on-the-fly data transforms?</a></li>
<li class="toctree-l1"><a class="reference internal" href="../FAQ.html#can-i-get-word-alignments-while-translating">Can I get word alignments while translating?</a></li>
</ul>
<p class="caption"><span class="caption-text">Examples</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Library.html">Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="Translation.html">Translation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Summarization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#preparing-the-data-and-vocab">Preparing the data and vocab</a></li>
<li class="toctree-l2"><a class="reference internal" href="#training">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="#inference">Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="#evaluation">Evaluation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#cnn-dm">CNN-DM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gigaword">Gigaword</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#scores-and-models">Scores and Models</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id1">CNN-DM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id2">Gigaword</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="GGNN.html">Gated Graph Sequence Neural Networks</a></li>
</ul>
<p class="caption"><span class="caption-text">Scripts</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../options/build_vocab.html">Build Vocab</a></li>
<li class="toctree-l1"><a class="reference internal" href="../options/train.html">Train</a></li>
<li class="toctree-l1"><a class="reference internal" href="../options/translate.html">Translate</a></li>
<li class="toctree-l1"><a class="reference internal" href="../options/server.html">Server</a></li>
</ul>
<p class="caption"><span class="caption-text">API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../onmt.html">Framework</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.modules.html">Modules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.translation.html">Translation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.translate.translation_server.html">Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../onmt.inputters.html">Data Loaders</a></li>
</ul>
<p class="caption"><span class="caption-text">Legacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../legacy/FAQ.html">FAQ (Legacy version)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legacy/im2text.html">Image to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legacy/speech2text.html">Speech to Text</a></li>
<li class="toctree-l1"><a class="reference internal" href="../legacy/vid2text.html">Video to Text</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">OpenNMT-py</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Summarization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/examples/Summarization.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="summarization">
<h1>Summarization<a class="headerlink" href="#summarization" title="Permalink to this headline">¶</a></h1>
<p>Note: The process and results below are presented in the paper <code class="docutils literal notranslate"><span class="pre">Bottom-Up</span> <span class="pre">Abstractive</span> <span class="pre">Summarization</span></code>. Please consider citing it if you follow these instructions.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="nd">@inproceedings</span><span class="p">{</span><span class="n">gehrmann2018bottom</span><span class="p">,</span>
  <span class="n">title</span><span class="o">=</span><span class="p">{</span><span class="n">Bottom</span><span class="o">-</span><span class="n">Up</span> <span class="n">Abstractive</span> <span class="n">Summarization</span><span class="p">},</span>
  <span class="n">author</span><span class="o">=</span><span class="p">{</span><span class="n">Gehrmann</span><span class="p">,</span> <span class="n">Sebastian</span> <span class="ow">and</span> <span class="n">Deng</span><span class="p">,</span> <span class="n">Yuntian</span> <span class="ow">and</span> <span class="n">Rush</span><span class="p">,</span> <span class="n">Alexander</span><span class="p">},</span>
  <span class="n">booktitle</span><span class="o">=</span><span class="p">{</span><span class="n">Proceedings</span> <span class="n">of</span> <span class="n">the</span> <span class="mi">2018</span> <span class="n">Conference</span> <span class="n">on</span> <span class="n">Empirical</span> <span class="n">Methods</span> <span class="ow">in</span> <span class="n">Natural</span> <span class="n">Language</span> <span class="n">Processing</span><span class="p">},</span>
  <span class="n">pages</span><span class="o">=</span><span class="p">{</span><span class="mi">4098</span><span class="o">--</span><span class="mi">4109</span><span class="p">},</span>
  <span class="n">year</span><span class="o">=</span><span class="p">{</span><span class="mi">2018</span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This document describes how to replicate summarization experiments on the CNN-DM and gigaword datasets using OpenNMT-py.
In the following, we assume access to a tokenized form of the corpus split into train/valid/test set. You can find the data <a class="reference external" href="https://github.com/harvardnlp/sent-summary">here</a>.</p>
<p>An example article-title pair from Gigaword should look like this:</p>
<p><strong>Input</strong>
<em>australia ‘s current account deficit shrunk by a record #.## billion dollars -lrb- #.## billion us -rrb- in the june quarter due to soaring commodity prices , figures released monday showed .</em></p>
<p><strong>Output</strong>
<em>australian current account deficit narrows sharply</em></p>
<div class="section" id="preparing-the-data-and-vocab">
<h2>Preparing the data and vocab<a class="headerlink" href="#preparing-the-data-and-vocab" title="Permalink to this headline">¶</a></h2>
<p>For CNN-DM we follow See et al. [2] and additionally truncate the source length at 400 tokens and the target at 100. We also note that in CNN-DM, we found models to work better if the target surrounds sentences with tags such that a sentence looks like <code class="docutils literal notranslate"><span class="pre">&lt;t&gt;</span> <span class="pre">w1</span> <span class="pre">w2</span> <span class="pre">w3</span> <span class="pre">.</span> <span class="pre">&lt;/t&gt;</span></code>. If you use this formatting, you can remove the tags after the inference step with the commands <code class="docutils literal notranslate"><span class="pre">sed</span> <span class="pre">-i</span> <span class="pre">'s/</span> <span class="pre">&lt;\/t&gt;//g'</span> <span class="pre">FILE.txt</span></code> and <code class="docutils literal notranslate"><span class="pre">sed</span> <span class="pre">-i</span> <span class="pre">'s/&lt;t&gt;</span> <span class="pre">//g'</span> <span class="pre">FILE.txt</span></code>.</p>
<p><strong>YAML Configuration</strong>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># cnndm.yaml</span>

<span class="c1">## Where the vocab(s) will be written</span>
<span class="nt">save_data</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/run/example</span>
<span class="c1"># Prevent overwriting existing files in the folder</span>
<span class="nt">overwrite</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>

<span class="c1"># truncate examples</span>
<span class="nt">src_seq_length_trunc</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">400</span>
<span class="nt">tgt_seq_length_trunc</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">100</span>

<span class="c1"># common vocabulary for source and target</span>
<span class="nt">share_vocab</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>

<span class="c1"># Corpus opts:</span>
<span class="nt">data</span><span class="p">:</span>
    <span class="nt">cnndm</span><span class="p">:</span>
        <span class="nt">path_src</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/train.txt.src</span>
        <span class="nt">path_tgt</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/train.txt.tgt.tagged</span>
    <span class="nt">valid</span><span class="p">:</span>
        <span class="nt">path_src</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/val.txt.src</span>
        <span class="nt">path_tgt</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/val.txt.tgt.tagged</span>
<span class="nn">...</span>
</pre></div>
</div>
<p>Let’s compute the vocab over the full dataset (<code class="docutils literal notranslate"><span class="pre">-n_sample</span> <span class="pre">-1</span></code>):</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>onmt_build_vocab -config cnndm.yaml -n_sample -1
</pre></div>
</div>
<p>This command will have written source and target vocabulary to <code class="docutils literal notranslate"><span class="pre">cnndm/run/example.vocab.src</span></code> and <code class="docutils literal notranslate"><span class="pre">cnndm/run/example.vocab.tgt</span></code>. These two files should be the same, as <code class="docutils literal notranslate"><span class="pre">share_vocab</span></code> is set.</p>
</div>
<div class="section" id="training">
<h2>Training<a class="headerlink" href="#training" title="Permalink to this headline">¶</a></h2>
<p>The training procedure described in this section for the most part follows parameter choices and implementation similar to that of See et al. [2].</p>
<p>Most significant options are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">copy_attn</span></code>: This is the most important option, since it allows the model to copy words from the source.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">global_attention</span> <span class="pre">mlp</span></code>: This makes the model use the  attention mechanism introduced by Bahdanau et al. [3] instead of that by Luong et al. [4] (<code class="docutils literal notranslate"><span class="pre">global_attention</span> <span class="pre">dot</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">share_embeddings</span></code>: This shares the word embeddings between encoder and decoder. This option drastically decreases the number of parameters a model has to learn. We did not find this option to helpful, but you can try it out by adding it to the command below.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reuse_copy_attn</span></code>: This option reuses the standard attention as copy attention. Without this, the model learns an additional attention that is only used for copying.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">copy_loss_by_seqlength</span></code>: This modifies the loss to divide the loss of a sequence by the number of tokens in it. In practice, we found this to generate longer sequences during inference. However, this effect can also be achieved by using penalties during decoding.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">bridge</span></code>: This is an additional layer that uses the final hidden state of the encoder as input and computes an initial hidden state for the decoder. Without this, the decoder is initialized with the final hidden state of the encoder directly.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">optim</span> <span class="pre">adagrad</span></code>: Adagrad outperforms SGD when coupled with the following option.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">adagrad_accumulator_init</span> <span class="pre">0.1</span></code>: PyTorch does not initialize the accumulator in adagrad with any values. To match the optimization algorithm with the Tensorflow version, this option needs to be added.</p></li>
</ul>
<p>Note: Since we are using copy-attention [1] in the model, additional fields will be computed so that source and target are aligned and use the same dictionary. Previously achieved with the <code class="docutils literal notranslate"><span class="pre">-dynamic_dict</span></code> preprocessing flag in the legacy version, this is now automatically handled when <code class="docutils literal notranslate"><span class="pre">-copy_attn</span></code> is enabled.</p>
<p>We are using using a 128-dimensional word-embedding, and 512-dimensional 1 layer LSTM. On the encoder side, we use a bidirectional LSTM (<code class="docutils literal notranslate"><span class="pre">brnn</span></code>), which means that the 512 dimensions are split into 256 dimensions per direction.</p>
<p>We additionally set the maximum norm of the gradient to 2, and renormalize if the gradient norm exceeds this value and do not use any dropout.</p>
<p><strong>Configurations</strong>:</p>
<p>(1) CNN-DM</p>
<p>The basic RNN configuration is defined by these parameters:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># maximum vocab size</span>
<span class="nt">src_vocab_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">50000</span>
<span class="nt">tgt_vocab_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">50000</span>

<span class="nt">src_vocab</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/run/example.vocab.src</span>
<span class="nt">tgt_vocab</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/run/example.vocab.tgt</span>

<span class="nt">save_model</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/run/model</span>
<span class="nt">copy_attn</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">global_attention</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">mlp</span>
<span class="nt">word_vec_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">128</span>
<span class="nt">rnn_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">512</span>
<span class="nt">layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="nt">encoder_type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">brnn</span>
<span class="nt">train_steps</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">200000</span>
<span class="nt">max_grad_norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">batch_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="nt">valid_batch_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">16</span>
<span class="nt">optim</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">adagrad</span>
<span class="nt">learning_rate</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.15</span>
<span class="nt">adagrad_accumulator_init</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="nt">reuse_copy_attn</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">copy_loss_by_seqlength</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">bridge</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">seed</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">777</span>
<span class="nt">world_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">gpu_ranks</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">0</span><span class="p p-Indicator">,</span> <span class="nv">1</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<p>(2) CNN-DM Transformer</p>
<p>Transformer configuration is the following:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">src_vocab_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">50000</span>
<span class="nt">tgt_vocab_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">50000</span>

<span class="nt">src_vocab</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/run/example.vocab.src</span>
<span class="nt">tgt_vocab</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/run/example.vocab.tgt</span>

<span class="nt">save_model</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">cnndm/run/model_transformer</span>
<span class="nt">layers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="nt">rnn_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">512</span>
<span class="nt">word_vec_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">512</span>
<span class="nt">max_grad_norm</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">optim</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">adam</span>
<span class="nt">encoder_type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">transformer</span>
<span class="nt">decoder_type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">transformer</span>
<span class="nt">position_encoding</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.2</span>
<span class="nt">attention_dropout</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.2</span>
<span class="nt">param_init</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
<span class="nt">warmup_steps</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">8000</span>
<span class="nt">learning_rate</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">decay_method</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">noam</span>
<span class="nt">label_smoothing</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.1</span>
<span class="nt">adam_beta2</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0.998</span>
<span class="nt">batch_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4096</span>
<span class="nt">batch_type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">tokens</span>
<span class="nt">normalization</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">tokens</span>
<span class="nt">train_steps</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">200000</span>
<span class="nt">accum_count</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">4</span>
<span class="nt">share_embeddings</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">copy_attn</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">param_init_glorot</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">world_size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="nt">gpu_ranks</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">0</span><span class="p p-Indicator">,</span> <span class="nv">1</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<p>(3) Gigaword</p>
<p>Gigaword can be trained equivalently. You just need to adapt the <code class="docutils literal notranslate"><span class="pre">data</span></code> part of the YAML configuration.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># gigaword.yaml</span>

<span class="c1">## Where the vocab(s) will be written</span>
<span class="nt">save_data</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">gigaword/run/example</span>
<span class="c1"># Prevent overwriting existing files in the folder</span>
<span class="nt">overwrite</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">False</span>

<span class="c1"># prevent filtering of long examples</span>
<span class="nt">src_seq_length</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10000</span>
<span class="nt">tgt_seq_length</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10000</span>

<span class="c1"># common vocabulary for source and target</span>
<span class="nt">share_vocab</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">True</span>

<span class="c1"># Corpus opts:</span>
<span class="nt">data</span><span class="p">:</span>
    <span class="nt">cnndm</span><span class="p">:</span>
        <span class="nt">path_src</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">gigaword/train.article.txt</span>
        <span class="nt">path_tgt</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">gigaword/train.title.txt</span>
        <span class="nt">transforms</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">filtertoolong</span><span class="p p-Indicator">]</span>
        <span class="nt">weight</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
    <span class="nt">valid</span><span class="p">:</span>
        <span class="nt">path_src</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">gigaword/valid.article.txt</span>
        <span class="nt">path_tgt</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">gigaword/valid.title.txt</span>
        <span class="nt">transforms</span><span class="p">:</span> <span class="p p-Indicator">[</span><span class="nv">filtertoolong</span><span class="p p-Indicator">]</span>
<span class="nn">...</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h2>
<p>During inference, we use beam-search with a beam-size of 10. We also added specific penalties that we can use during decoding, described in the following.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">stepwise_penalty</span></code>: Applies penalty at every step</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">coverage_penalty</span> <span class="pre">summary</span></code>: Uses a penalty that prevents repeated attention to the same source word</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">beta</span> <span class="pre">5</span></code>: Parameter for the Coverage Penalty</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">length_penalty</span> <span class="pre">wu</span></code>: Uses the Length Penalty by Wu et al.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">alpha</span> <span class="pre">0.8</span></code>: Parameter for the Length Penalty.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">block_ngram_repeat</span> <span class="pre">3</span></code>: Prevent the model from repeating trigrams.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ignore_when_blocking</span> <span class="pre">&quot;.&quot;</span> <span class="pre">&quot;&lt;/t&gt;&quot;</span> <span class="pre">&quot;&lt;t&gt;&quot;</span></code>: Allow the model to repeat trigrams with the sentence boundary tokens.</p></li>
</ul>
<p><strong>Commands used</strong>:</p>
<p>(1) CNN-DM</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">onmt_translate</span> <span class="o">-</span><span class="n">gpu</span> <span class="n">X</span> \
               <span class="o">-</span><span class="n">batch_size</span> <span class="mi">20</span> \
               <span class="o">-</span><span class="n">beam_size</span> <span class="mi">10</span> \
               <span class="o">-</span><span class="n">model</span> <span class="n">cnndm</span><span class="o">/</span><span class="n">run</span><span class="o">/...</span> \
               <span class="o">-</span><span class="n">src</span> <span class="n">cnndm</span><span class="o">/</span><span class="n">test</span><span class="o">.</span><span class="n">txt</span><span class="o">.</span><span class="n">src</span> \
               <span class="o">-</span><span class="n">output</span> <span class="n">testout</span><span class="o">/</span><span class="n">cnndm</span><span class="o">.</span><span class="n">out</span> \
               <span class="o">-</span><span class="n">min_length</span> <span class="mi">35</span> \
               <span class="o">-</span><span class="n">verbose</span> \
               <span class="o">-</span><span class="n">stepwise_penalty</span> \
               <span class="o">-</span><span class="n">coverage_penalty</span> <span class="n">summary</span> \
               <span class="o">-</span><span class="n">beta</span> <span class="mi">5</span> \
               <span class="o">-</span><span class="n">length_penalty</span> <span class="n">wu</span> \
               <span class="o">-</span><span class="n">alpha</span> <span class="mf">0.9</span> \
               <span class="o">-</span><span class="n">verbose</span> \
               <span class="o">-</span><span class="n">block_ngram_repeat</span> <span class="mi">3</span> \
               <span class="o">-</span><span class="n">ignore_when_blocking</span> <span class="s2">&quot;.&quot;</span> <span class="s2">&quot;&lt;/t&gt;&quot;</span> <span class="s2">&quot;&lt;t&gt;&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="evaluation">
<h2>Evaluation<a class="headerlink" href="#evaluation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="cnn-dm">
<h3>CNN-DM<a class="headerlink" href="#cnn-dm" title="Permalink to this headline">¶</a></h3>
<p>To evaluate the ROUGE scores on CNN-DM, we extended the <code class="docutils literal notranslate"><span class="pre">pyrouge</span></code> wrapper with additional evaluations such as the amount of repeated n-grams (typically found in models with copy attention), found <a class="reference external" href="https://github.com/sebastianGehrmann/rouge-baselines">here</a>. The repository includes a sub-repo called pyrouge. Make sure to clone the code with the <code class="docutils literal notranslate"><span class="pre">git</span> <span class="pre">clone</span> <span class="pre">--recurse-submodules</span> <span class="pre">https://github.com/sebastianGehrmann/rouge-baselines</span></code> command to check this out as well and follow the installation instructions on the pyrouge repository before calling this script.
The installation instructions can be found <a class="reference external" href="https://github.com/falcondai/pyrouge/tree/9cdbfbda8b8d96e7c2646ffd048743ddcf417ed9#installation">here</a>. Note that on MacOS, we found that the pointer to your perl installation in line 1 of <code class="docutils literal notranslate"><span class="pre">pyrouge/RELEASE-1.5.5/ROUGE-1.5.5.pl</span></code> might be different from the one you have installed. A simple fix is to change this line to <code class="docutils literal notranslate"><span class="pre">#!/usr/local/bin/perl</span> <span class="pre">-w</span></code> if it fails.</p>
<p>It can be run with the following command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">baseline</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">s</span> <span class="n">testout</span><span class="o">/</span><span class="n">cnndm</span><span class="o">.</span><span class="n">out</span> <span class="o">-</span><span class="n">t</span> <span class="n">data</span><span class="o">/</span><span class="n">cnndm</span><span class="o">/</span><span class="n">test</span><span class="o">.</span><span class="n">txt</span><span class="o">.</span><span class="n">tgt</span><span class="o">.</span><span class="n">tagged</span> <span class="o">-</span><span class="n">m</span> <span class="n">sent_tag_verbatim</span> <span class="o">-</span><span class="n">r</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">sent_tag_verbatim</span></code> option strips <code class="docutils literal notranslate"><span class="pre">&lt;t&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;/t&gt;</span></code> tags around sentences - when a sentence previously was <code class="docutils literal notranslate"><span class="pre">&lt;t&gt;</span> <span class="pre">w</span> <span class="pre">w</span> <span class="pre">w</span> <span class="pre">w</span> <span class="pre">.</span> <span class="pre">&lt;/t&gt;</span></code>, it becomes <code class="docutils literal notranslate"><span class="pre">w</span> <span class="pre">w</span> <span class="pre">w</span> <span class="pre">w</span> <span class="pre">.</span></code>.</p>
</div>
<div class="section" id="gigaword">
<h3>Gigaword<a class="headerlink" href="#gigaword" title="Permalink to this headline">¶</a></h3>
<p>For evaluation of large test sets such as Gigaword, we use the a parallel python wrapper around ROUGE, found <a class="reference external" href="https://github.com/pltrdy/files2rouge">here</a>.</p>
<p><strong>Command used</strong>:
<code class="docutils literal notranslate"><span class="pre">files2rouge</span> <span class="pre">giga.out</span> <span class="pre">test.title.txt</span> <span class="pre">--verbose</span></code></p>
</div>
</div>
<div class="section" id="scores-and-models">
<h2>Scores and Models<a class="headerlink" href="#scores-and-models" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>CNN-DM<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model Type</th>
<th>Model</th>
<th align="right">R1 R</th>
<th align="right">R1 P</th>
<th align="right">R1 F</th>
<th align="right">R2 R</th>
<th align="right">R2 P</th>
<th align="right">R2 F</th>
<th align="right">RL R</th>
<th align="right">RL P</th>
<th align="right">RL F</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pointer-Generator + Coverage [2]</td>
<td><a href="https://github.com/abisee/pointer-generator">link</a></td>
<td align="right">39.05</td>
<td align="right">43.02</td>
<td align="right">39.53</td>
<td align="right">17.16</td>
<td align="right">18.77</td>
<td align="right">17.28</td>
<td align="right">35.98</td>
<td align="right">39.56</td>
<td align="right">36.38</td>
</tr>
<tr>
<td>Pointer-Generator [2]</td>
<td><a href="https://github.com/abisee/pointer-generator">link</a></td>
<td align="right">37.76</td>
<td align="right">37.60</td>
<td align="right">36.44</td>
<td align="right">16.31</td>
<td align="right">16.12</td>
<td align="right">15.66</td>
<td align="right">34.66</td>
<td align="right">34.46</td>
<td align="right">33.42</td>
</tr>
<tr>
<td>OpenNMT BRNN  (1 layer, emb 128, hid 512)</td>
<td><a href="https://s3.amazonaws.com/opennmt-models/Summary/ada6_bridge_oldcopy_tagged_acc_54.17_ppl_11.17_e20.pt">link</a></td>
<td align="right">40.90</td>
<td align="right">40.20</td>
<td align="right">39.02</td>
<td align="right">17.91</td>
<td align="right">17.99</td>
<td align="right">17.25</td>
<td align="right">37.76</td>
<td align="right">37.18</td>
<td align="right">36.05</td>
</tr>
<tr>
<td>OpenNMT BRNN  (1 layer, emb 128, hid 512, shared embeddings)</td>
<td><a href="https://s3.amazonaws.com/opennmt-models/Summary/ada6_bridge_oldcopy_tagged_share_acc_54.50_ppl_10.89_e20.pt">link</a></td>
<td align="right">38.59</td>
<td align="right">40.60</td>
<td align="right">37.97</td>
<td align="right">16.75</td>
<td align="right">17.93</td>
<td align="right">16.59</td>
<td align="right">35.67</td>
<td align="right">37.60</td>
<td align="right">35.13</td>
</tr>
<tr>
<td>OpenNMT BRNN (2 layer, emb 256, hid 1024)</td>
<td><a href="https://s3.amazonaws.com/opennmt-models/Summary/ada6_bridge_oldcopy_tagged_larger_acc_54.84_ppl_10.58_e17.pt">link</a></td>
<td align="right">40.41</td>
<td align="right">40.94</td>
<td align="right">39.12</td>
<td align="right">17.76</td>
<td align="right">18.38</td>
<td align="right">17.35</td>
<td align="right">37.27</td>
<td align="right">37.83</td>
<td align="right">36.12</td>
</tr>
<tr>
<td>OpenNMT Transformer</td>
<td><a href="https://s3.amazonaws.com/opennmt-models/sum_transformer_model_acc_57.25_ppl_9.22_e16.pt">link</a></td>
<td align="right">40.31</td>
<td align="right">41.09</td>
<td align="right">39.25</td>
<td align="right">17.97</td>
<td align="right">18.46</td>
<td align="right">17.54</td>
<td align="right">37.41</td>
<td align="right">38.18</td>
<td align="right">36.45</td>
</tr>
</tbody>
</table></div>
<div class="section" id="id2">
<h3>Gigaword<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<table border="1" class="docutils">
<thead>
<tr>
<th>Model Type</th>
<th>Model</th>
<th align="right">R1 R</th>
<th align="right">R1 P</th>
<th align="right">R1 F</th>
<th align="right">R2 R</th>
<th align="right">R2 P</th>
<th align="right">R2 F</th>
<th align="right">RL R</th>
<th align="right">RL P</th>
<th align="right">RL F</th>
</tr>
</thead>
<tbody>
<tr>
<td>OpenNMT, no penalties</td>
<td><a href="https://s3.amazonaws.com/opennmt-models/gigaword_copy_acc_51.78_ppl_11.71_e20.pt">link</a></td>
<td align="right">?</td>
<td align="right">?</td>
<td align="right">35.51</td>
<td align="right">?</td>
<td align="right">?</td>
<td align="right">17.35</td>
<td align="right">?</td>
<td align="right">?</td>
<td align="right">33.17</td>
</tr>
</tbody>
</table></div>
</div>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<p>[1] Vinyals, O., Fortunato, M. and Jaitly, N., 2015. Pointer Network. NIPS</p>
<p>[2] See, A., Liu, P.J. and Manning, C.D., 2017. Get To The Point: Summarization with Pointer-Generator Networks. ACL</p>
<p>[3] Bahdanau, D., Cho, K. and Bengio, Y., 2014. Neural machine translation by jointly learning to align and translate. ICLR</p>
<p>[4] Luong, M.T., Pham, H. and Manning, C.D., 2015. Effective approaches to attention-based neural machine translation. EMNLP</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="GGNN.html" class="btn btn-neutral float-right" title="Gated Graph Sequence Neural Networks" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="Translation.html" class="btn btn-neutral float-left" title="Translation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, srush

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>